{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping pagina Web Colombia TIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Reportes de Colombia TIC\n",
    "\n",
    "Este notebook permite automatizar la descarga de reportes de Colombia TIC, extraer y procesar datos relevantes, \n",
    "y finalmente combinar la información en un solo DataFrame para análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias necesarias para el proyecto\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título de la página de prueba: Inicio\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la URL base para la descarga y el uso de carpetas de almacenamiento.\n",
    "\n",
    "BASE_URL = \"https://colombiatic.mintic.gov.co/679/w3-channel.html\"\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "combined_data_path = \"reporteCombinado\"\n",
    "\n",
    "# ### Configuración de Selenium WebDriver\n",
    "\n",
    "# Configurar el servicio de Chrome con la ruta completa\n",
    "service = Service(\"C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe\")\n",
    "\n",
    "# Opciones de Chrome (opcional, para correr en modo headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(BASE_URL)\n",
    "    print(\"Título de la página de prueba:\", driver.title)\n",
    "except Exception as e:\n",
    "    print(\"Error al inicializar ChromeDriver:\", e)\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Función para crear subcarpetas\n",
    "\n",
    "def create_directories(base_path, year, trimester):\n",
    "    \"\"\"\n",
    "    Crea directorios para almacenar los archivos según año y trimestre.\n",
    "    \"\"\"\n",
    "    path = os.path.join(base_path, f\"Año_{year}\", f\"Trimestre_{trimester}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ### Función para descargar el reporte\n",
    "\n",
    "def download_report(url, save_path):\n",
    "    \"\"\"\n",
    "    Descarga el archivo Excel desde la URL proporcionada y lo guarda en el path especificado.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(save_path, \"reporte_tic_trimestral.xlsx\")\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Reporte descargado en {filename}\")\n",
    "    return filename\n",
    "\n",
    "# ## Descarga y Almacenamiento del Reporte\n",
    "\n",
    "def archivo_trimestral_existente(nombre_archivo):\n",
    "    return os.path.exists(nombre_archivo)\n",
    "\n",
    "# Función principal para descargar el reporte\n",
    "# Definir la ruta de la carpeta donde se guardará el archivo descargado\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "\n",
    "def fetch_report(url):\n",
    "    \"\"\"\n",
    "    Navega en la página de Colombia TIC, encuentra y descarga el archivo Excel más reciente.\n",
    "    \"\"\"\n",
    "    # Configurar el servicio de Chrome\n",
    "    service = Service('C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe')\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Esperar y encontrar la sección \"Destacados\"\n",
    "        destacados_header = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h2[@class='ntg-titulo-caja' and text()='Destacados']\"))\n",
    "        )\n",
    "        print(\"Se encontró la sección 'Destacados'.\")\n",
    "\n",
    "        # Buscar el enlace al artículo en la sección \"Destacados\"\n",
    "        destacados_section = driver.find_element(By.XPATH, \"//div[@class='recuadro col-md-6 col-lg-4 text-center mb-4']\")\n",
    "        link_element = destacados_section.find_element(By.TAG_NAME, \"a\")\n",
    "        report_url = link_element.get_attribute('href')\n",
    "\n",
    "        print(\"URL del artículo:\", report_url)\n",
    "\n",
    "        # Extraer el trimestre y año del reporte a partir del texto del enlace\n",
    "        trimestre_texto = link_element.text\n",
    "        trimestre = re.search(r\"(primer|segundo|tercer|cuarto) trimestre de (\\d{4})\", trimestre_texto, re.IGNORECASE)\n",
    "        if trimestre:\n",
    "            trimestre_nombre = trimestre.group(1)\n",
    "            año = trimestre.group(2)\n",
    "            nombre_archivo = f\"reporte_tic_{trimestre_nombre}_trimestre_{año}.xlsx\"\n",
    "        else:\n",
    "            print(\"No se pudo determinar el trimestre del reporte.\")\n",
    "            nombre_archivo = \"reporte_tic_trimestral.xlsx\"\n",
    "\n",
    "        # Crear la carpeta de descarga si no existe\n",
    "        os.makedirs(download_base_path, exist_ok=True)\n",
    "        \n",
    "        # Ruta completa donde se guardará el archivo\n",
    "        archivo_path = os.path.join(download_base_path, nombre_archivo)\n",
    "        \n",
    "        # Verificar si el archivo del trimestre ya existe\n",
    "        if os.path.exists(archivo_path):\n",
    "            print(f\"El archivo '{archivo_path}' ya existe. No se realizará la descarga.\")\n",
    "            return archivo_path\n",
    "        else:\n",
    "            # Navegar a la página del artículo\n",
    "            driver.get(report_url)\n",
    "\n",
    "            # Esperar a que el `<span>` que contiene el enlace al archivo Excel esté presente\n",
    "            excel_span = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//span[contains(@class, 'bajardoc') and contains(@class, 'binary-archivo_xls') and contains(@class, 'format-xlsx')]\"))\n",
    "            )\n",
    "\n",
    "            # Obtener el enlace al archivo Excel\n",
    "            excel_link = excel_span.find_element(By.TAG_NAME, \"a\")\n",
    "            excel_url = excel_link.get_attribute('href')\n",
    "            print(\"URL del archivo Excel:\", excel_url)\n",
    "\n",
    "            # Descargar el archivo Excel usando requests\n",
    "            excel_response = requests.get(excel_url)\n",
    "\n",
    "            # Verificar el código de estado antes de guardar\n",
    "            if excel_response.status_code == 200:\n",
    "                with open(archivo_path, 'wb') as file:\n",
    "                    file.write(excel_response.content)\n",
    "                print(f\"Reporte descargado exitosamente como {archivo_path}\")\n",
    "                return archivo_path\n",
    "            else:\n",
    "                print(f\"Error al descargar el archivo. Código de estado: {excel_response.status_code}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"No se encontró la sección 'Destacados' o hubo un error:\", str(e))\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Función para renombrar columnas duplicadas con sufijos secuenciales\n",
    "def rename_duplicates(columns):\n",
    "    \"\"\"\n",
    "    Renombra columnas duplicadas añadiendo un sufijo '_dupN' para evitar conflictos de nombres.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "def save_large_df_to_excel(df, file_path, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en un archivo Excel, dividiéndolo en múltiples hojas si es necesario.\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame a guardar.\n",
    "        file_path (str): Ruta del archivo Excel de salida.\n",
    "        chunk_size (int): Número máximo de filas por hoja.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            df_chunk = df.iloc[i:i + chunk_size]\n",
    "            sheet_name = f\"Sheet_{i // chunk_size + 1}\"\n",
    "            df_chunk.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo combinado final guardado en {file_path}\")\n",
    "\n",
    "def save_large_df_to_csv(df, base_path, chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en múltiples archivos CSV en una subcarpeta especificada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df: DataFrame a guardar.\n",
    "    - base_path: Ruta base para guardar los archivos CSV.\n",
    "    - chunk_size: Tamaño del lote en número de filas para dividir el archivo CSV si es grande.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que la carpeta base exista\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Dividir y guardar el DataFrame en archivos CSV en chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "        file_path = os.path.join(base_path, f\"combined_data_part_{i // chunk_size + 1}.csv\")\n",
    "        chunk.to_csv(file_path, index=False)\n",
    "        print(f\"Guardado el archivo {file_path}\")\n",
    "\n",
    "# ## Procesamiento del Reporte\n",
    "\n",
    "def process_report2(file_path):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del archivo descargado, aplica limpieza y estandarización de columnas.\n",
    "    \"\"\"\n",
    "    # Definir las columnas esperadas para cada hoja\n",
    "    column_structure = {\n",
    "        '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "        '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "        '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "        '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "        '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "        '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "        '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "        '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "        '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "        '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "        '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "    }\n",
    "\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    processed_sheets = {}\n",
    "    \n",
    "    # Función para renombrar columnas duplicadas\n",
    "    def rename_duplicates(columns):\n",
    "        counts = {}\n",
    "        new_columns = []\n",
    "        for col in columns:\n",
    "            if col in counts:\n",
    "                counts[col] += 1\n",
    "                new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "            else:\n",
    "                counts[col] = 0\n",
    "                new_columns.append(col)\n",
    "        return new_columns\n",
    "\n",
    "    # Limpiar cada hoja y ajustar las columnas\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        if sheet_name not in column_structure:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' (sin estructura conocida)\")\n",
    "            continue\n",
    "\n",
    "        # Limpiar encabezados y resetear índices\n",
    "        df = df.rename(columns=lambda x: x.strip()).dropna(how='all')\n",
    "        \n",
    "        # Renombrar columnas duplicadas\n",
    "        df.columns = rename_duplicates(df.columns)\n",
    "\n",
    "        # Validar si el número de columnas coincide\n",
    "        if len(df.columns) == len(column_structure[sheet_name]):\n",
    "            df.columns = column_structure[sheet_name]\n",
    "            processed_sheets[sheet_name] = df\n",
    "        else:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' debido a la longitud de columnas no coincidente.\")\n",
    "    \n",
    "    # Guardar un archivo temporal para depuración\n",
    "    temp_output_path = os.path.join(\"reporteCombinado\", \"temp_combined_debug.xlsx\")\n",
    "    with pd.ExcelWriter(temp_output_path) as writer:\n",
    "        for sheet_name, df in processed_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo temporal guardado en: {temp_output_path}\")\n",
    "\n",
    "    # Combinar todas las hojas en un solo DataFrame\n",
    "    combined_df = processed_sheets.pop('1')\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        # Verificar y renombrar columnas duplicadas que podrían causar problemas en el merge\n",
    "        for col in df.columns:\n",
    "            if col in combined_df.columns and col != 'AÑO' and col != 'TRIMESTRE':\n",
    "                df = df.rename(columns={col: f\"{col}_{sheet_name}\"})\n",
    "\n",
    "        # Obtener columnas comunes\n",
    "        common_columns = combined_df.columns.intersection(df.columns).tolist()\n",
    "        combined_df = pd.merge(combined_df, df, on=common_columns, how='outer', suffixes=('', f'_{sheet_name}'))\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_report3(file_path, temp_folder=\"reporteCombinado/temp_lotes\", min_column_count=5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja de un archivo Excel, la guarda en archivos temporales por lotes y luego combina los resultados.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel a procesar.\n",
    "        temp_folder (str): Carpeta para guardar los archivos procesados temporalmente.\n",
    "        min_column_count (int): Número mínimo de columnas para procesar una hoja.\n",
    "        batch_size (int): Número de archivos en cada lote para la combinación por lotes.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame combinado de todas las hojas procesadas.\n",
    "    \"\"\"\n",
    "    # Crear la carpeta temporal para guardar archivos procesados\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "    # Cargar el archivo Excel\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Paso 1: Procesa cada hoja y guarda el resultado individualmente\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = excel_file.parse(sheet_name)\n",
    "            \n",
    "            # Verificar la estructura de la hoja antes de procesar\n",
    "            if len(df.columns) < min_column_count:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' debido a longitud de columnas insuficiente.\")\n",
    "                continue\n",
    "            \n",
    "            # Limpiar los nombres de las columnas y renombrar duplicados\n",
    "            df.columns = [col.strip() for col in df.columns]  # Quitar espacios en blanco\n",
    "            df.columns = rename_duplicates(df.columns)\n",
    "            \n",
    "            # Guardar cada hoja procesada en un archivo temporal CSV\n",
    "            temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "            df.to_csv(temp_file_path, index=False)\n",
    "            print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "    # Paso 2: Combina los archivos temporales en lotes\n",
    "    # Lista de archivos temporales procesados\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    # DataFrame combinado final\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Combinación por lotes\n",
    "    for i in range(0, len(temp_files), batch_size):\n",
    "        batch_files = temp_files[i:i + batch_size]\n",
    "        \n",
    "        # Leer y combinar el lote actual\n",
    "        batch_df = pd.concat([pd.read_csv(f) for f in batch_files], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Combina el lote con el DataFrame final\n",
    "        if not combined_df.empty:\n",
    "            common_columns = combined_df.columns.intersection(batch_df.columns).tolist()\n",
    "            combined_df = pd.merge(combined_df, batch_df, on=common_columns, how='outer', suffixes=('', '_dup'))\n",
    "        else:\n",
    "            combined_df = batch_df.copy()  # Inicializar con el primer lote\n",
    "        \n",
    "        print(f\"Lote {i // batch_size + 1} combinado exitosamente.\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# ## Guardar el DataFrame Combinado\n",
    "\n",
    "def save_combined_data(df, path):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame combinado en un archivo Excel en la carpeta 'reporteCombinado'.\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(path, \"data_reporte_combined_final.xlsx\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"DataFrame combinado guardado en: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ## Análisis y Validación de Datos\n",
    "\n",
    "# ### Función para verificar valores únicos y posibles inconsistencias\n",
    "\n",
    "def analyze_columns(df):\n",
    "    \"\"\"\n",
    "    Muestra valores únicos y verifica consistencias en las columnas clave.\n",
    "    \"\"\"\n",
    "    columnas_clave = ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET']\n",
    "    \n",
    "    # Mostrar valores únicos en columnas clave\n",
    "    for col in columnas_clave:\n",
    "        if col in df.columns:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"\\nValores únicos en '{col}':\", unique_values[:10])  # Muestra los primeros 10 valores únicos\n",
    "\n",
    "# ### Función para identificar y manejar datos nulos\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identifica y maneja valores nulos en el DataFrame, aplicando reemplazos si es necesario.\n",
    "    \"\"\"\n",
    "    missing_info = df.isnull().sum()\n",
    "    print(\"\\nValores nulos en cada columna:\\n\", missing_info)\n",
    "    \n",
    "    # Puedes aplicar reemplazos según lo necesario, por ejemplo, reemplazar NaN en algunas columnas\n",
    "    df['No. ACCESOS FIJOS A INTERNET'] = df['No. ACCESOS FIJOS A INTERNET'].fillna(0)\n",
    "    # Más reemplazos pueden añadirse aquí según los análisis\n",
    "    return df\n",
    "\n",
    "# ### Función para generar estadísticas descriptivas\n",
    "\n",
    "def generate_statistics(df):\n",
    "    \"\"\"\n",
    "    Genera estadísticas descriptivas para las columnas numéricas del DataFrame.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    print(\"\\nEstadísticas descriptivas de columnas numéricas:\\n\", df[numeric_cols].describe())\n",
    "\n",
    "# ### Guardar versión final del DataFrame para análisis\n",
    "\n",
    "def save_final_data(df):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame limpio y validado en un archivo Excel en la subcarpeta /reporteCombinado para análisis final.\n",
    "    \"\"\"\n",
    "    final_output_path = os.path.join(combined_data_path, \"data_reporte_combined_validated.xlsx\")\n",
    "    df.to_excel(final_output_path, index=False)\n",
    "    print(f\"DataFrame final validado guardado en: {final_output_path}\")\n",
    "\n",
    "\n",
    "# Carpeta para los archivos procesados temporalmente\n",
    "temp_folder = \"reporteCombinado/temp_lotes\"\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "# Mínimo número de columnas esperadas para cada hoja (ajustar según tu estructura)\n",
    "cierto_numero_de_columnas_minimo = 4\n",
    "\n",
    "# Estructura esperada de columnas por hoja (añadir según tu estructura)\n",
    "column_structure = {\n",
    "    '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "    '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "    '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "    '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "    '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "    '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "    '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "    '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "    '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "    '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "    '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "}\n",
    "# Función para limpiar y estandarizar cada hoja\n",
    "# Función para limpiar y estandarizar cada hoja\n",
    "def limpiar_hoja(df, columnas_esperadas):\n",
    "    \"\"\"\n",
    "    Limpia una hoja eliminando encabezados irrelevantes y ajustando las columnas.\n",
    "    Args:\n",
    "        df (DataFrame): Hoja leída del Excel.\n",
    "        columnas_esperadas (list): Lista de columnas esperadas.\n",
    "    Returns:\n",
    "        DataFrame: Hoja limpiada.\n",
    "    \"\"\"\n",
    "    # Eliminar filas completamente vacías\n",
    "    df = df.dropna(how=\"all\")\n",
    "    \n",
    "    # Intentar detectar la fila donde comienzan los datos relevantes\n",
    "    valid_row_index = None\n",
    "    for i, row in df.iterrows():\n",
    "        if row.notna().sum() >= len(columnas_esperadas) - 2:  # Flexibilidad para detectar columnas faltantes\n",
    "            valid_row_index = i\n",
    "            break\n",
    "    \n",
    "    if valid_row_index is not None:\n",
    "        # Cortar el DataFrame desde la fila detectada\n",
    "        df = df.iloc[valid_row_index:].reset_index(drop=True)\n",
    "        \n",
    "        # Reasignar los nombres de las columnas a partir de las esperadas\n",
    "        df.columns = columnas_esperadas[: len(df.columns)]  # Ajustar columnas al número detectado\n",
    "        df = df.reset_index(drop=True)\n",
    "    else:\n",
    "        print(f\"No se encontraron filas válidas en la hoja con columnas esperadas: {columnas_esperadas}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Detectar todas las columnas únicas en el archivo\n",
    "def detect_all_columns(file_path, column_structure):\n",
    "    \"\"\"\n",
    "    Detecta todas las columnas únicas en un archivo Excel basándose en la estructura definida.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel.\n",
    "        column_structure (dict): Estructura esperada de las columnas.\n",
    "    Returns:\n",
    "        list: Lista de todas las columnas únicas en el archivo.\n",
    "    \"\"\"\n",
    "    all_columns = set()\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        try:\n",
    "            # Verifica si la hoja está en la estructura esperada\n",
    "            if sheet_name in column_structure:\n",
    "                all_columns.update(column_structure[sheet_name])\n",
    "            else:\n",
    "                print(f\"Omitiendo hoja '{sheet_name}' porque no está en la estructura.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "    return list(all_columns)\n",
    "\n",
    "# Procesar y combinar reporte\n",
    "def process_report(file_path, column_structure, temp_folder=\"reporteCombinado/temp_lotes\"):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del reporte, limpia y alinea columnas, y combina el resultado en un archivo CSV final.\n",
    "    \"\"\"\n",
    "    # Detectar todas las columnas únicas basadas en column_structure\n",
    "    all_columns = detect_all_columns(file_path, column_structure)\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    # Procesamiento y almacenamiento de cada hoja\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        if sheet_name in column_structure:\n",
    "            try:\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                cleaned_df = limpiar_hoja(df, column_structure[sheet_name])\n",
    "                \n",
    "                # Guardar cada hoja procesada en un archivo temporal CSV\n",
    "                temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "                cleaned_df.to_csv(temp_file_path, index=False)\n",
    "                print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' (estructura desconocida)\")\n",
    "\n",
    "    # Combinación de los archivos CSV temporales en un archivo final\n",
    "    combined_data_path = \"reporteCombinado/combined_data_final.csv\"\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    with open(combined_data_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        for i, temp_file in enumerate(temp_files):\n",
    "            batch_df = pd.read_csv(temp_file, dtype=str, low_memory=False)\n",
    "            batch_df = batch_df.reindex(columns=all_columns, fill_value=\"\")  # Alinear todas las columnas\n",
    "            \n",
    "            # Concatenar archivos con encabezado solo en el primer archivo\n",
    "            batch_df.to_csv(output_file, mode='a', header=(i == 0), index=False)\n",
    "            print(f\"Archivo {temp_file} concatenado exitosamente en {combined_data_path}\")\n",
    "\n",
    "    print(f\"Archivo combinado final guardado en {combined_data_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_report_con_hojas_4x(file_path, column_structure, temp_folder=\"reporteCombinado/temp_lotes\"):\n",
    "    \"\"\"\n",
    "    Procesa todas las hojas del archivo Excel, eliminando encabezados irrelevantes y ajustando las columnas.\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "            # Verificar si la hoja está en la estructura esperada\n",
    "            if sheet_name in column_structure:\n",
    "                cleaned_df = limpiar_hoja(df, column_structure[sheet_name])\n",
    "            else:\n",
    "                print(f\"Omitiendo hoja '{sheet_name}' porque no está en la estructura.\")\n",
    "                continue\n",
    "\n",
    "            # Guardar la hoja procesada en un archivo temporal CSV\n",
    "            temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "            cleaned_df.to_csv(temp_file_path, index=False)\n",
    "            print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "\n",
    "def process_report_con_hojas_4x(file_path, column_structure, temp_folder=\"reporteCombinado/temp_lotes\"):\n",
    "    \"\"\"\n",
    "    Procesa todas las hojas del archivo Excel aplicando limpieza especial para las hojas '4,x'.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel a procesar.\n",
    "        column_structure (dict): Estructura esperada de las columnas.\n",
    "        temp_folder (str): Carpeta donde se guardarán los archivos procesados temporalmente.\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "            # Verificar si la hoja está en la estructura esperada\n",
    "            if sheet_name in column_structure:\n",
    "                cleaned_df = limpiar_hoja(df, column_structure[sheet_name])\n",
    "                \n",
    "                # Guardar la hoja procesada en un archivo temporal CSV\n",
    "                temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "                cleaned_df.to_csv(temp_file_path, index=False)\n",
    "                print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "            else:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' porque no tiene estructura conocida.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "def deduplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Deduplica nombres de columnas en el DataFrame.\n",
    "    \"\"\"\n",
    "    df.columns = pd.io.parsers.ParserBase({'names': df.columns})._maybe_dedup_names(df.columns)\n",
    "    return df\n",
    "\n",
    "def flujo_completo(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Flujo completo para procesar el reporte TIC.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel.\n",
    "        output_path (str): Carpeta donde se guardará el dataset final.\n",
    "    \"\"\"\n",
    "    # Define las columnas esperadas\n",
    "    column_structure = {\n",
    "        '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "        '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "        '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "        '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "        '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "        '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "        '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "        '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "        '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "        '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "        '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "    }\n",
    "\n",
    "    # Detectar todas las columnas únicas\n",
    "    all_columns = detect_all_columns(file_path, column_structure)\n",
    "\n",
    "    # Procesar el reporte con limpieza y validación\n",
    "    temp_folder = os.path.join(output_path, \"temp_lotes\")\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "            # Validar si está en la estructura esperada\n",
    "            if sheet_name in column_structure:\n",
    "                cleaned_df = limpiar_hoja(df, column_structure[sheet_name])\n",
    "\n",
    "                # Eliminar encabezados duplicados (si aparecen en los datos)\n",
    "                if (cleaned_df.columns == column_structure[sheet_name]).all():\n",
    "                    cleaned_df = cleaned_df.loc[~(cleaned_df == cleaned_df.columns).all(axis=1)]\n",
    "\n",
    "                # Guardar como archivo temporal\n",
    "                temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "                cleaned_df.to_csv(temp_file_path, index=False)\n",
    "                print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "            else:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' porque no está en la estructura conocida.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Combinar los archivos temporales procesados\n",
    "    combined_data_path = os.path.join(output_path, \"combined_data_final.csv\")\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    # Inicializar el DataFrame combinado\n",
    "    combined_df = None\n",
    "\n",
    "    for temp_file in temp_files:\n",
    "        # Leer el archivo temporal\n",
    "        current_df = pd.read_csv(temp_file, dtype=str, low_memory=False)\n",
    "\n",
    "        if combined_df is None:\n",
    "            # Si es el primer archivo, inicializamos el DataFrame combinado\n",
    "            combined_df = current_df\n",
    "        else:\n",
    "            # Realizar merge usando las columnas comunes como clave\n",
    "            common_columns = ['AÑO', 'TRIMESTRE', 'PROVEEDOR']\n",
    "            combined_df = pd.merge(combined_df, current_df, on=common_columns, how='outer', suffixes=('', '_dup'))\n",
    "\n",
    "    # Guardar el DataFrame combinado en un archivo final\n",
    "    combined_df.to_csv(combined_data_path, index=False, encoding='utf-8')\n",
    "    print(f\"Archivo combinado final guardado en {combined_data_path}\")\n",
    "\n",
    "    return combined_df\n",
    "    \n",
    "def debug_sheet_names(file_path, column_structure):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    sheet_names = excel_file.sheet_names\n",
    "    print(f\"Nombres de hojas en el archivo: {sheet_names}\")\n",
    "    print(f\"Nombres esperados en column_structure: {list(column_structure.keys())}\")    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def flujo_completo_por_lotes(file_path, output_path, column_structure, chunk_size=500_000):\n",
    "    \"\"\"\n",
    "    Flujo completo para procesar el reporte TIC y combinarlo en un dataset final usando chunks para evitar errores de memoria.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel.\n",
    "        output_path (str): Carpeta donde se guardará el dataset final.\n",
    "        chunk_size (int): Tamaño máximo de filas para procesar en cada lote.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_folder = os.path.join(output_path, \"temp_lotes\")\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    # Procesar cada hoja en archivos temporales\n",
    "    process_report_con_hojas_4x(file_path, column_structure, temp_folder=temp_folder)\n",
    "\n",
    "    # Obtener archivos temporales procesados\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    # Inicializar el archivo de salida incremental\n",
    "    combined_data_path = os.path.join(output_path, \"combined_data_final.csv\")\n",
    "    header_written = False\n",
    "\n",
    "    for temp_file in temp_files:\n",
    "        # Leer el archivo temporal por chunks\n",
    "        for chunk in pd.read_csv(temp_file, dtype=str, chunksize=chunk_size):\n",
    "            if not header_written:\n",
    "                chunk.to_csv(combined_data_path, mode='w', index=False, header=True)\n",
    "                header_written = True\n",
    "            else:\n",
    "                chunk.to_csv(combined_data_path, mode='a', index=False, header=False)\n",
    "\n",
    "    print(f\"Archivo combinado final guardado en {combined_data_path}\")\n",
    "    return combined_data_path\n",
    "\n",
    "def flujo_completo_optimizado(file_path, output_path, column_structure, batch_size=5):\n",
    "    \"\"\"\n",
    "    Flujo completo para procesar y combinar reportes en bloques para evitar problemas de memoria.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel.\n",
    "        output_path (str): Carpeta para el dataset final.\n",
    "        batch_size (int): Número de archivos temporales para procesar en cada bloque.\n",
    "    \"\"\"\n",
    "    # Paso 1: Procesar cada hoja y guardar como archivos temporales\n",
    "    \n",
    "    temp_folder = os.path.join(output_path, \"temp_lotes\")\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    process_report_con_hojas_4x(file_path, column_structure, temp_folder=temp_folder)\n",
    "\n",
    "    # Paso 2: Combinar por lotes y guardar resultados intermedios\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "    intermediate_combined_path = os.path.join(output_path, \"intermediate_combined.csv\")\n",
    "\n",
    "    # Inicializar DataFrame vacío para combinación por lotes\n",
    "    with open(intermediate_combined_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        for i in range(0, len(temp_files), batch_size):\n",
    "            batch_files = temp_files[i:i + batch_size]\n",
    "            batch_df = pd.concat([pd.read_csv(f, dtype=str) for f in batch_files], axis=0, ignore_index=True)\n",
    "\n",
    "            # Reorganizar columnas únicas y escribir en archivo intermedio\n",
    "            if i == 0:\n",
    "                all_columns = list(set(batch_df.columns))\n",
    "                batch_df = batch_df.reindex(columns=all_columns, fill_value='')\n",
    "                batch_df.to_csv(output_file, mode='w', index=False, header=True)\n",
    "            else:\n",
    "                batch_df = batch_df.reindex(columns=all_columns, fill_value='')\n",
    "                batch_df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "            print(f\"Lote {i // batch_size + 1} combinado y guardado temporalmente.\")\n",
    "\n",
    "    # Paso 3: Leer archivo combinado intermedio y guardar resultado final\n",
    "    final_output_path = os.path.join(output_path, \"combined_data_final.csv\")\n",
    "    final_df = pd.read_csv(intermediate_combined_path, dtype=str)\n",
    "    final_df.to_csv(final_output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Archivo combinado final guardado en {final_output_path}\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio del programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontró la sección 'Destacados'.\n",
      "URL del artículo: https://colombiatic.mintic.gov.co/679/w3-article-397520.html\n",
      "El archivo 'reporteColombiaTic\\reporte_tic_segundo_trimestre_2024.xlsx' ya existe. No se realizará la descarga.\n"
     ]
    }
   ],
   "source": [
    "# ## Ejecución del Script\n",
    "\n",
    "# Descargar y procesar el reporte\n",
    "file_path = fetch_report(BASE_URL)  # Descarga y guarda el archivo en subcarpeta /reporteColombiaTic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de hojas en el archivo: ['PORTADA', 'CONTENIDO', '1', '2', '3', '4,1', '4,2', '4,3', '4,4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
      "Nombres esperados en column_structure: ['1', '2', '3', '4,1', '4,2', '4,3', '4,4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17']\n"
     ]
    }
   ],
   "source": [
    "debug_sheet_names(file_path, column_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define las columnas esperadas\n",
    "column_structure = {\n",
    "        '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "        '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "        '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "        '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "        '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "        '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "        '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "        '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "        '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "        '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "        '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitiendo la hoja 'PORTADA' porque no tiene estructura conocida.\n",
      "Omitiendo la hoja 'CONTENIDO' porque no tiene estructura conocida.\n",
      "Hoja '1' procesada y guardada en reporteCombinado\\temp_lotes\\1_processed.csv\n",
      "Hoja '2' procesada y guardada en reporteCombinado\\temp_lotes\\2_processed.csv\n",
      "Hoja '3' procesada y guardada en reporteCombinado\\temp_lotes\\3_processed.csv\n",
      "Hoja '4,1' procesada y guardada en reporteCombinado\\temp_lotes\\4,1_processed.csv\n",
      "Hoja '4,2' procesada y guardada en reporteCombinado\\temp_lotes\\4,2_processed.csv\n",
      "Hoja '4,3' procesada y guardada en reporteCombinado\\temp_lotes\\4,3_processed.csv\n",
      "Hoja '4,4' procesada y guardada en reporteCombinado\\temp_lotes\\4,4_processed.csv\n",
      "Hoja '5' procesada y guardada en reporteCombinado\\temp_lotes\\5_processed.csv\n",
      "Hoja '6' procesada y guardada en reporteCombinado\\temp_lotes\\6_processed.csv\n",
      "Hoja '7' procesada y guardada en reporteCombinado\\temp_lotes\\7_processed.csv\n",
      "Hoja '8' procesada y guardada en reporteCombinado\\temp_lotes\\8_processed.csv\n",
      "Hoja '9' procesada y guardada en reporteCombinado\\temp_lotes\\9_processed.csv\n",
      "Hoja '10' procesada y guardada en reporteCombinado\\temp_lotes\\10_processed.csv\n",
      "Hoja '11' procesada y guardada en reporteCombinado\\temp_lotes\\11_processed.csv\n",
      "Hoja '12' procesada y guardada en reporteCombinado\\temp_lotes\\12_processed.csv\n",
      "Hoja '13' procesada y guardada en reporteCombinado\\temp_lotes\\13_processed.csv\n",
      "Hoja '14' procesada y guardada en reporteCombinado\\temp_lotes\\14_processed.csv\n",
      "Hoja '15' procesada y guardada en reporteCombinado\\temp_lotes\\15_processed.csv\n",
      "Hoja '16' procesada y guardada en reporteCombinado\\temp_lotes\\16_processed.csv\n",
      "Hoja '17' procesada y guardada en reporteCombinado\\temp_lotes\\17_processed.csv\n",
      "Omitiendo la hoja '18' porque no tiene estructura conocida.\n",
      "Lote 1 combinado y guardado temporalmente.\n",
      "Lote 2 combinado y guardado temporalmente.\n",
      "Lote 3 combinado y guardado temporalmente.\n",
      "Lote 4 combinado y guardado temporalmente.\n",
      "Lote 5 combinado y guardado temporalmente.\n",
      "Archivo combinado final guardado en reporteCombinado\\combined_data_final.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"reporteCombinado\"  # Carpeta donde se guardará el dataset final\n",
    "#all_columns = detect_all_columns(file_path, column_structure)\n",
    "df_final=flujo_completo_optimizado(file_path, output_path, column_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josvaldes\\AppData\\Local\\Temp\\ipykernel_18416\\1606582176.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasetFinal = pd.read_csv(\"reporteCombinado/combined_data_final.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INGRESOS</th>\n",
       "      <th>TRÁFICO POSPAGO</th>\n",
       "      <th>CONSUMO PREPAGO</th>\n",
       "      <th>LÍNEAS POSPAGO</th>\n",
       "      <th>TERMINAL</th>\n",
       "      <th>AÑO</th>\n",
       "      <th>TRÁFICO (MB)</th>\n",
       "      <th>TRÁFICO PREPAGO</th>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <th>LÍNEAS EN SERVICIO</th>\n",
       "      <th>LÍNEAS ACTIVADAS</th>\n",
       "      <th>INGRESOS OPERACIONALES</th>\n",
       "      <th>CONSUMO POSPAGO</th>\n",
       "      <th>PROVEEDOR</th>\n",
       "      <th>LÍNEAS PREPAGO</th>\n",
       "      <th>PROVEEDOR DESTINO</th>\n",
       "      <th>LÍNEAS RETIRADAS</th>\n",
       "      <th>SEGMENTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INGRESOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TERMINAL</td>\n",
       "      <td>AÑO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRIMESTRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROVEEDOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEGMENTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>676051722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TELÉFONO MÓVIL</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMPRESAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7936553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DATA CARD</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMPRESAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DATA CARD</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSONAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5444337923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TELÉFONO MÓVIL</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSONAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     INGRESOS TRÁFICO POSPAGO CONSUMO PREPAGO LÍNEAS POSPAGO        TERMINAL   \n",
       "0    INGRESOS             NaN             NaN            NaN        TERMINAL  \\\n",
       "1   676051722             NaN             NaN            NaN  TELÉFONO MÓVIL   \n",
       "2     7936553             NaN             NaN            NaN       DATA CARD   \n",
       "3     1241491             NaN             NaN            NaN       DATA CARD   \n",
       "4  5444337923             NaN             NaN            NaN  TELÉFONO MÓVIL   \n",
       "\n",
       "    AÑO TRÁFICO (MB) TRÁFICO PREPAGO  TRIMESTRE LÍNEAS EN SERVICIO   \n",
       "0   AÑO          NaN             NaN  TRIMESTRE                NaN  \\\n",
       "1  2021          NaN             NaN          4                NaN   \n",
       "2  2021          NaN             NaN          4                NaN   \n",
       "3  2021          NaN             NaN          4                NaN   \n",
       "4  2021          NaN             NaN          4                NaN   \n",
       "\n",
       "  LÍNEAS ACTIVADAS INGRESOS OPERACIONALES CONSUMO POSPAGO      PROVEEDOR   \n",
       "0              NaN                    NaN             NaN      PROVEEDOR  \\\n",
       "1              NaN                    NaN             NaN  AVANTEL S.A.S   \n",
       "2              NaN                    NaN             NaN  AVANTEL S.A.S   \n",
       "3              NaN                    NaN             NaN  AVANTEL S.A.S   \n",
       "4              NaN                    NaN             NaN  AVANTEL S.A.S   \n",
       "\n",
       "  LÍNEAS PREPAGO PROVEEDOR DESTINO LÍNEAS RETIRADAS  SEGMENTO  \n",
       "0            NaN               NaN              NaN  SEGMENTO  \n",
       "1            NaN               NaN              NaN  EMPRESAS  \n",
       "2            NaN               NaN              NaN  EMPRESAS  \n",
       "3            NaN               NaN              NaN  PERSONAS  \n",
       "4            NaN               NaN              NaN  PERSONAS  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetFinal = pd.read_csv(\"reporteCombinado/combined_data_final.csv\")   \n",
    "datasetFinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INGRESOS</th>\n",
       "      <th>TRÁFICO POSPAGO</th>\n",
       "      <th>CONSUMO PREPAGO</th>\n",
       "      <th>LÍNEAS POSPAGO</th>\n",
       "      <th>TERMINAL</th>\n",
       "      <th>AÑO</th>\n",
       "      <th>TRÁFICO (MB)</th>\n",
       "      <th>TRÁFICO PREPAGO</th>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <th>LÍNEAS EN SERVICIO</th>\n",
       "      <th>LÍNEAS ACTIVADAS</th>\n",
       "      <th>INGRESOS OPERACIONALES</th>\n",
       "      <th>CONSUMO POSPAGO</th>\n",
       "      <th>PROVEEDOR</th>\n",
       "      <th>LÍNEAS PREPAGO</th>\n",
       "      <th>PROVEEDOR DESTINO</th>\n",
       "      <th>LÍNEAS RETIRADAS</th>\n",
       "      <th>SEGMENTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INGRESOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TERMINAL</td>\n",
       "      <td>AÑO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRIMESTRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROVEEDOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEGMENTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>676051722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TELÉFONO MÓVIL</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMPRESAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7936553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DATA CARD</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMPRESAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DATA CARD</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSONAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5444337923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TELÉFONO MÓVIL</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVANTEL S.A.S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSONAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269550</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269551</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269552</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269553</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2269554 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           INGRESOS TRÁFICO POSPAGO CONSUMO PREPAGO LÍNEAS POSPAGO   \n",
       "0          INGRESOS             NaN             NaN            NaN  \\\n",
       "1         676051722             NaN             NaN            NaN   \n",
       "2           7936553             NaN             NaN            NaN   \n",
       "3           1241491             NaN             NaN            NaN   \n",
       "4        5444337923             NaN             NaN            NaN   \n",
       "...             ...             ...             ...            ...   \n",
       "2269549         NaN             NaN             NaN            NaN   \n",
       "2269550         NaN             NaN             NaN            NaN   \n",
       "2269551         NaN             NaN             NaN            NaN   \n",
       "2269552         NaN             NaN             NaN            NaN   \n",
       "2269553         NaN             NaN             NaN            NaN   \n",
       "\n",
       "               TERMINAL   AÑO TRÁFICO (MB) TRÁFICO PREPAGO  TRIMESTRE   \n",
       "0              TERMINAL   AÑO          NaN             NaN  TRIMESTRE  \\\n",
       "1        TELÉFONO MÓVIL  2021          NaN             NaN          4   \n",
       "2             DATA CARD  2021          NaN             NaN          4   \n",
       "3             DATA CARD  2021          NaN             NaN          4   \n",
       "4        TELÉFONO MÓVIL  2021          NaN             NaN          4   \n",
       "...                 ...   ...          ...             ...        ...   \n",
       "2269549             NaN   NaN          NaN             NaN        NaN   \n",
       "2269550             NaN   NaN          NaN             NaN        NaN   \n",
       "2269551             NaN   NaN          NaN             NaN        NaN   \n",
       "2269552             NaN   NaN          NaN             NaN        NaN   \n",
       "2269553             NaN   NaN          NaN             NaN        NaN   \n",
       "\n",
       "        LÍNEAS EN SERVICIO LÍNEAS ACTIVADAS INGRESOS OPERACIONALES   \n",
       "0                      NaN              NaN                    NaN  \\\n",
       "1                      NaN              NaN                    NaN   \n",
       "2                      NaN              NaN                    NaN   \n",
       "3                      NaN              NaN                    NaN   \n",
       "4                      NaN              NaN                    NaN   \n",
       "...                    ...              ...                    ...   \n",
       "2269549                NaN              NaN                    NaN   \n",
       "2269550                NaN              NaN                    NaN   \n",
       "2269551                NaN              NaN                    NaN   \n",
       "2269552                NaN              NaN                    NaN   \n",
       "2269553                NaN              NaN                    NaN   \n",
       "\n",
       "        CONSUMO POSPAGO      PROVEEDOR LÍNEAS PREPAGO PROVEEDOR DESTINO   \n",
       "0                   NaN      PROVEEDOR            NaN               NaN  \\\n",
       "1                   NaN  AVANTEL S.A.S            NaN               NaN   \n",
       "2                   NaN  AVANTEL S.A.S            NaN               NaN   \n",
       "3                   NaN  AVANTEL S.A.S            NaN               NaN   \n",
       "4                   NaN  AVANTEL S.A.S            NaN               NaN   \n",
       "...                 ...            ...            ...               ...   \n",
       "2269549             NaN            NaN            NaN               NaN   \n",
       "2269550             NaN            NaN            NaN               NaN   \n",
       "2269551             NaN            NaN            NaN               NaN   \n",
       "2269552             NaN            NaN            NaN               NaN   \n",
       "2269553             NaN            NaN            NaN               NaN   \n",
       "\n",
       "        LÍNEAS RETIRADAS  SEGMENTO  \n",
       "0                    NaN  SEGMENTO  \n",
       "1                    NaN  EMPRESAS  \n",
       "2                    NaN  EMPRESAS  \n",
       "3                    NaN  PERSONAS  \n",
       "4                    NaN  PERSONAS  \n",
       "...                  ...       ...  \n",
       "2269549              NaN       NaN  \n",
       "2269550              NaN       NaN  \n",
       "2269551              NaN       NaN  \n",
       "2269552              NaN       NaN  \n",
       "2269553              NaN       NaN  \n",
       "\n",
       "[2269554 rows x 18 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el dataset final: ['INGRESOS', 'TRÁFICO POSPAGO', 'CONSUMO PREPAGO', 'LÍNEAS POSPAGO', 'TERMINAL', 'AÑO', 'TRÁFICO (MB)', 'TRÁFICO PREPAGO', 'TRIMESTRE', 'LÍNEAS EN SERVICIO', 'LÍNEAS ACTIVADAS', 'INGRESOS OPERACIONALES', 'CONSUMO POSPAGO', 'PROVEEDOR', 'LÍNEAS PREPAGO', 'PROVEEDOR DESTINO', 'LÍNEAS RETIRADAS', 'SEGMENTO']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas en el dataset final:\", datasetFinal.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df = process_report(file_path, column_structure)  # Procesa y combina las hojas del reporte\n",
    "\n",
    "# Guardar el DataFrame combinado en formato CSV\n",
    "#combined_data_path_csv = \"reporteCombinado/combined_data_final.csv\"\n",
    "#combined_df.to_csv(combined_data_path_csv, index=False)\n",
    "#print(f\"Archivo combinado final guardado en {combined_data_path_csv}\")\n",
    "\n",
    "#combined_data_path = \"reporteCombinado\"\n",
    "#save_large_df_to_csv(combined_df, combined_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[153], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Guardar el DataFrame combinado en un archivo final\u001b[39;00m\n\u001b[0;32m      2\u001b[0m combined_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreporteCombinado/combined_data_final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m(combined_data_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo combinado final guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_excel'"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame combinado en un archivo final\n",
    "combined_data_path = \"reporteCombinado/combined_data_final.xlsx\"\n",
    "combined_df.to_excel(combined_data_path, index=False)\n",
    "print(f\"Archivo combinado final guardado en {combined_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un archivo Excel en la subcarpeta /reporteCombinado\n",
    "save_combined_data(combined_df, combined_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Ejecución de Validación y Análisis\n",
    "\n",
    "# Realizar análisis de columnas clave\n",
    "analyze_columns(combined_df)\n",
    "\n",
    "# Manejar valores nulos\n",
    "combined_df = handle_missing_values(combined_df)\n",
    "\n",
    "# Generar estadísticas descriptivas\n",
    "generate_statistics(combined_df)\n",
    "\n",
    "# Guardar la versión final del DataFrame validado\n",
    "save_final_data(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
