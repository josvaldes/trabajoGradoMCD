{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping pagina Web Colombia TIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Reportes de Colombia TIC\n",
    "\n",
    "Este notebook permite automatizar la descarga de reportes de Colombia TIC, extraer y procesar datos relevantes, \n",
    "y finalmente combinar la información en un solo DataFrame para análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias necesarias para el proyecto\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título de la página de prueba: Inicio\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la URL base para la descarga y el uso de carpetas de almacenamiento.\n",
    "\n",
    "BASE_URL = \"https://colombiatic.mintic.gov.co/679/w3-channel.html\"\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "combined_data_path = \"reporteCombinado\"\n",
    "\n",
    "# ### Configuración de Selenium WebDriver\n",
    "\n",
    "# Configurar el servicio de Chrome con la ruta completa\n",
    "service = Service(\"C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe\")\n",
    "\n",
    "# Opciones de Chrome (opcional, para correr en modo headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(BASE_URL)\n",
    "    print(\"Título de la página de prueba:\", driver.title)\n",
    "except Exception as e:\n",
    "    print(\"Error al inicializar ChromeDriver:\", e)\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Función para crear subcarpetas\n",
    "\n",
    "def create_directories(base_path, year, trimester):\n",
    "    \"\"\"\n",
    "    Crea directorios para almacenar los archivos según año y trimestre.\n",
    "    \"\"\"\n",
    "    path = os.path.join(base_path, f\"Año_{year}\", f\"Trimestre_{trimester}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ### Función para descargar el reporte\n",
    "\n",
    "def download_report(url, save_path):\n",
    "    \"\"\"\n",
    "    Descarga el archivo Excel desde la URL proporcionada y lo guarda en el path especificado.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(save_path, \"reporte_tic_trimestral.xlsx\")\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Reporte descargado en {filename}\")\n",
    "    return filename\n",
    "\n",
    "# ## Descarga y Almacenamiento del Reporte\n",
    "\n",
    "def archivo_trimestral_existente(nombre_archivo):\n",
    "    return os.path.exists(nombre_archivo)\n",
    "\n",
    "# Función principal para descargar el reporte\n",
    "# Definir la ruta de la carpeta donde se guardará el archivo descargado\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "\n",
    "def fetch_report(url):\n",
    "    \"\"\"\n",
    "    Navega en la página de Colombia TIC, encuentra y descarga el archivo Excel más reciente.\n",
    "    \"\"\"\n",
    "    # Configurar el servicio de Chrome\n",
    "    service = Service('C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe')\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Esperar y encontrar la sección \"Destacados\"\n",
    "        destacados_header = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h2[@class='ntg-titulo-caja' and text()='Destacados']\"))\n",
    "        )\n",
    "        print(\"Se encontró la sección 'Destacados'.\")\n",
    "\n",
    "        # Buscar el enlace al artículo en la sección \"Destacados\"\n",
    "        destacados_section = driver.find_element(By.XPATH, \"//div[@class='recuadro col-md-6 col-lg-4 text-center mb-4']\")\n",
    "        link_element = destacados_section.find_element(By.TAG_NAME, \"a\")\n",
    "        report_url = link_element.get_attribute('href')\n",
    "\n",
    "        print(\"URL del artículo:\", report_url)\n",
    "\n",
    "        # Extraer el trimestre y año del reporte a partir del texto del enlace\n",
    "        trimestre_texto = link_element.text\n",
    "        trimestre = re.search(r\"(primer|segundo|tercer|cuarto) trimestre de (\\d{4})\", trimestre_texto, re.IGNORECASE)\n",
    "        if trimestre:\n",
    "            trimestre_nombre = trimestre.group(1)\n",
    "            año = trimestre.group(2)\n",
    "            nombre_archivo = f\"reporte_tic_{trimestre_nombre}_trimestre_{año}.xlsx\"\n",
    "        else:\n",
    "            print(\"No se pudo determinar el trimestre del reporte.\")\n",
    "            nombre_archivo = \"reporte_tic_trimestral.xlsx\"\n",
    "\n",
    "        # Crear la carpeta de descarga si no existe\n",
    "        os.makedirs(download_base_path, exist_ok=True)\n",
    "        \n",
    "        # Ruta completa donde se guardará el archivo\n",
    "        archivo_path = os.path.join(download_base_path, nombre_archivo)\n",
    "        \n",
    "        # Verificar si el archivo del trimestre ya existe\n",
    "        if os.path.exists(archivo_path):\n",
    "            print(f\"El archivo '{archivo_path}' ya existe. No se realizará la descarga.\")\n",
    "            return archivo_path\n",
    "        else:\n",
    "            # Navegar a la página del artículo\n",
    "            driver.get(report_url)\n",
    "\n",
    "            # Esperar a que el `<span>` que contiene el enlace al archivo Excel esté presente\n",
    "            excel_span = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//span[contains(@class, 'bajardoc') and contains(@class, 'binary-archivo_xls') and contains(@class, 'format-xlsx')]\"))\n",
    "            )\n",
    "\n",
    "            # Obtener el enlace al archivo Excel\n",
    "            excel_link = excel_span.find_element(By.TAG_NAME, \"a\")\n",
    "            excel_url = excel_link.get_attribute('href')\n",
    "            print(\"URL del archivo Excel:\", excel_url)\n",
    "\n",
    "            # Descargar el archivo Excel usando requests\n",
    "            excel_response = requests.get(excel_url)\n",
    "\n",
    "            # Verificar el código de estado antes de guardar\n",
    "            if excel_response.status_code == 200:\n",
    "                with open(archivo_path, 'wb') as file:\n",
    "                    file.write(excel_response.content)\n",
    "                print(f\"Reporte descargado exitosamente como {archivo_path}\")\n",
    "                return archivo_path\n",
    "            else:\n",
    "                print(f\"Error al descargar el archivo. Código de estado: {excel_response.status_code}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"No se encontró la sección 'Destacados' o hubo un error:\", str(e))\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Función para renombrar columnas duplicadas con sufijos secuenciales\n",
    "def rename_duplicates(columns):\n",
    "    \"\"\"\n",
    "    Renombra columnas duplicadas añadiendo un sufijo '_dupN' para evitar conflictos de nombres.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "def save_large_df_to_excel(df, file_path, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en un archivo Excel, dividiéndolo en múltiples hojas si es necesario.\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame a guardar.\n",
    "        file_path (str): Ruta del archivo Excel de salida.\n",
    "        chunk_size (int): Número máximo de filas por hoja.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            df_chunk = df.iloc[i:i + chunk_size]\n",
    "            sheet_name = f\"Sheet_{i // chunk_size + 1}\"\n",
    "            df_chunk.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo combinado final guardado en {file_path}\")\n",
    "\n",
    "def save_large_df_to_csv(df, base_path, chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en múltiples archivos CSV en una subcarpeta especificada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df: DataFrame a guardar.\n",
    "    - base_path: Ruta base para guardar los archivos CSV.\n",
    "    - chunk_size: Tamaño del lote en número de filas para dividir el archivo CSV si es grande.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que la carpeta base exista\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Dividir y guardar el DataFrame en archivos CSV en chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "        file_path = os.path.join(base_path, f\"combined_data_part_{i // chunk_size + 1}.csv\")\n",
    "        chunk.to_csv(file_path, index=False)\n",
    "        print(f\"Guardado el archivo {file_path}\")\n",
    "\n",
    "# ## Procesamiento del Reporte\n",
    "\n",
    "def process_report2(file_path):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del archivo descargado, aplica limpieza y estandarización de columnas.\n",
    "    \"\"\"\n",
    "    # Definir las columnas esperadas para cada hoja\n",
    "    column_structure = {\n",
    "        '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "        '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "        '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "        '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "        '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "        '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "        '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "        '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "        '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "        '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "        '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "    }\n",
    "\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    processed_sheets = {}\n",
    "    \n",
    "    # Función para renombrar columnas duplicadas\n",
    "    def rename_duplicates(columns):\n",
    "        counts = {}\n",
    "        new_columns = []\n",
    "        for col in columns:\n",
    "            if col in counts:\n",
    "                counts[col] += 1\n",
    "                new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "            else:\n",
    "                counts[col] = 0\n",
    "                new_columns.append(col)\n",
    "        return new_columns\n",
    "\n",
    "    # Limpiar cada hoja y ajustar las columnas\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        if sheet_name not in column_structure:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' (sin estructura conocida)\")\n",
    "            continue\n",
    "\n",
    "        # Limpiar encabezados y resetear índices\n",
    "        df = df.rename(columns=lambda x: x.strip()).dropna(how='all')\n",
    "        \n",
    "        # Renombrar columnas duplicadas\n",
    "        df.columns = rename_duplicates(df.columns)\n",
    "\n",
    "        # Validar si el número de columnas coincide\n",
    "        if len(df.columns) == len(column_structure[sheet_name]):\n",
    "            df.columns = column_structure[sheet_name]\n",
    "            processed_sheets[sheet_name] = df\n",
    "        else:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' debido a la longitud de columnas no coincidente.\")\n",
    "    \n",
    "    # Guardar un archivo temporal para depuración\n",
    "    temp_output_path = os.path.join(\"reporteCombinado\", \"temp_combined_debug.xlsx\")\n",
    "    with pd.ExcelWriter(temp_output_path) as writer:\n",
    "        for sheet_name, df in processed_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo temporal guardado en: {temp_output_path}\")\n",
    "\n",
    "    # Combinar todas las hojas en un solo DataFrame\n",
    "    combined_df = processed_sheets.pop('1')\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        # Verificar y renombrar columnas duplicadas que podrían causar problemas en el merge\n",
    "        for col in df.columns:\n",
    "            if col in combined_df.columns and col != 'AÑO' and col != 'TRIMESTRE':\n",
    "                df = df.rename(columns={col: f\"{col}_{sheet_name}\"})\n",
    "\n",
    "        # Obtener columnas comunes\n",
    "        common_columns = combined_df.columns.intersection(df.columns).tolist()\n",
    "        combined_df = pd.merge(combined_df, df, on=common_columns, how='outer', suffixes=('', f'_{sheet_name}'))\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_report3(file_path, temp_folder=\"reporteCombinado/temp_lotes\", min_column_count=5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja de un archivo Excel, la guarda en archivos temporales por lotes y luego combina los resultados.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel a procesar.\n",
    "        temp_folder (str): Carpeta para guardar los archivos procesados temporalmente.\n",
    "        min_column_count (int): Número mínimo de columnas para procesar una hoja.\n",
    "        batch_size (int): Número de archivos en cada lote para la combinación por lotes.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame combinado de todas las hojas procesadas.\n",
    "    \"\"\"\n",
    "    # Crear la carpeta temporal para guardar archivos procesados\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "    # Cargar el archivo Excel\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Paso 1: Procesa cada hoja y guarda el resultado individualmente\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = excel_file.parse(sheet_name)\n",
    "            \n",
    "            # Verificar la estructura de la hoja antes de procesar\n",
    "            if len(df.columns) < min_column_count:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' debido a longitud de columnas insuficiente.\")\n",
    "                continue\n",
    "            \n",
    "            # Limpiar los nombres de las columnas y renombrar duplicados\n",
    "            df.columns = [col.strip() for col in df.columns]  # Quitar espacios en blanco\n",
    "            df.columns = rename_duplicates(df.columns)\n",
    "            \n",
    "            # Guardar cada hoja procesada en un archivo temporal CSV\n",
    "            temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "            df.to_csv(temp_file_path, index=False)\n",
    "            print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "    # Paso 2: Combina los archivos temporales en lotes\n",
    "    # Lista de archivos temporales procesados\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    # DataFrame combinado final\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Combinación por lotes\n",
    "    for i in range(0, len(temp_files), batch_size):\n",
    "        batch_files = temp_files[i:i + batch_size]\n",
    "        \n",
    "        # Leer y combinar el lote actual\n",
    "        batch_df = pd.concat([pd.read_csv(f) for f in batch_files], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Combina el lote con el DataFrame final\n",
    "        if not combined_df.empty:\n",
    "            common_columns = combined_df.columns.intersection(batch_df.columns).tolist()\n",
    "            combined_df = pd.merge(combined_df, batch_df, on=common_columns, how='outer', suffixes=('', '_dup'))\n",
    "        else:\n",
    "            combined_df = batch_df.copy()  # Inicializar con el primer lote\n",
    "        \n",
    "        print(f\"Lote {i // batch_size + 1} combinado exitosamente.\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# ## Guardar el DataFrame Combinado\n",
    "\n",
    "def save_combined_data(df, path):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame combinado en un archivo Excel en la carpeta 'reporteCombinado'.\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(path, \"data_reporte_combined_final.xlsx\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"DataFrame combinado guardado en: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ## Análisis y Validación de Datos\n",
    "\n",
    "# ### Función para verificar valores únicos y posibles inconsistencias\n",
    "\n",
    "def analyze_columns(df):\n",
    "    \"\"\"\n",
    "    Muestra valores únicos y verifica consistencias en las columnas clave.\n",
    "    \"\"\"\n",
    "    columnas_clave = ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET']\n",
    "    \n",
    "    # Mostrar valores únicos en columnas clave\n",
    "    for col in columnas_clave:\n",
    "        if col in df.columns:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"\\nValores únicos en '{col}':\", unique_values[:10])  # Muestra los primeros 10 valores únicos\n",
    "\n",
    "# ### Función para identificar y manejar datos nulos\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identifica y maneja valores nulos en el DataFrame, aplicando reemplazos si es necesario.\n",
    "    \"\"\"\n",
    "    missing_info = df.isnull().sum()\n",
    "    print(\"\\nValores nulos en cada columna:\\n\", missing_info)\n",
    "    \n",
    "    # Puedes aplicar reemplazos según lo necesario, por ejemplo, reemplazar NaN en algunas columnas\n",
    "    df['No. ACCESOS FIJOS A INTERNET'] = df['No. ACCESOS FIJOS A INTERNET'].fillna(0)\n",
    "    # Más reemplazos pueden añadirse aquí según los análisis\n",
    "    return df\n",
    "\n",
    "# ### Función para generar estadísticas descriptivas\n",
    "\n",
    "def generate_statistics(df):\n",
    "    \"\"\"\n",
    "    Genera estadísticas descriptivas para las columnas numéricas del DataFrame.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    print(\"\\nEstadísticas descriptivas de columnas numéricas:\\n\", df[numeric_cols].describe())\n",
    "\n",
    "# ### Guardar versión final del DataFrame para análisis\n",
    "\n",
    "def save_final_data(df):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame limpio y validado en un archivo Excel en la subcarpeta /reporteCombinado para análisis final.\n",
    "    \"\"\"\n",
    "    final_output_path = os.path.join(combined_data_path, \"data_reporte_combined_validated.xlsx\")\n",
    "    df.to_excel(final_output_path, index=False)\n",
    "    print(f\"DataFrame final validado guardado en: {final_output_path}\")\n",
    "\n",
    "\n",
    "# Carpeta para los archivos procesados temporalmente\n",
    "temp_folder = \"reporteCombinado/temp_lotes\"\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "# Mínimo número de columnas esperadas para cada hoja (ajustar según tu estructura)\n",
    "cierto_numero_de_columnas_minimo = 4\n",
    "\n",
    "# Estructura esperada de columnas por hoja (añadir según tu estructura)\n",
    "column_structure = {\n",
    "    '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "    '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "    '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "    '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "    '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "    '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "    '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "    '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "    '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "    '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "    '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "}\n",
    "\n",
    "def limpiar_hoja(df, columnas_esperadas):\n",
    "    \"\"\"\n",
    "    Limpia una hoja eliminando encabezados adicionales y filas vacías, \n",
    "    y aplica la estructura de columnas esperada.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='all')  # Elimina filas completamente vacías\n",
    "    df.columns = df.columns.str.strip()  # Limpia espacios en nombres de columnas\n",
    "\n",
    "    # Encuentra la primera fila válida de datos\n",
    "    valid_row_index = df[df.iloc[:, 0].astype(str).str.match(r'^\\d{4}$')].index.min()\n",
    "    if valid_row_index is not None:\n",
    "        df = df.iloc[valid_row_index:]\n",
    "        df.columns = columnas_esperadas  # Aplica las columnas esperadas\n",
    "        df = df.reset_index(drop=True)  # Reinicia el índice\n",
    "    else:\n",
    "        print(f\"No se encontraron filas válidas en la hoja con columnas {columnas_esperadas}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_report(file_path):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del archivo descargado, guarda cada una como CSV temporal,\n",
    "    y luego combina todos los CSV temporales en un archivo final.\n",
    "    \"\"\"\n",
    "    # Cargar cada hoja individualmente para evitar errores de archivos ZIP\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        try:\n",
    "            # Intenta leer la hoja\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            # Verifica si el número de columnas es suficiente\n",
    "            if len(df.columns) < cierto_numero_de_columnas_minimo:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' debido a longitud de columnas insuficiente.\")\n",
    "                continue\n",
    "\n",
    "            # Limpiar la hoja si tiene columnas esperadas\n",
    "            if sheet_name in column_structure:\n",
    "                columnas_esperadas = column_structure[sheet_name]\n",
    "                if len(df.columns) == len(columnas_esperadas):\n",
    "                    df = limpiar_hoja(df, columnas_esperadas)\n",
    "                else:\n",
    "                    print(f\"Omitiendo la hoja '{sheet_name}' debido a la longitud de columnas no coincidente.\")\n",
    "                    continue\n",
    "\n",
    "            # Guarda la hoja procesada en un archivo CSV temporal\n",
    "            temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "            df.to_csv(temp_file_path, index=False)\n",
    "            print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "    # Combina los archivos CSV temporales por lotes\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "    batch_size = 10  # Tamaño del lote de combinación para manejar memoria\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(temp_files), batch_size):\n",
    "        batch_files = temp_files[i:i + batch_size]\n",
    "        batch_df = pd.concat([pd.read_csv(f, low_memory=False) for f in batch_files], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Realiza la combinación por lotes\n",
    "        common_columns = combined_df.columns.intersection(batch_df.columns).tolist() if not combined_df.empty else batch_df.columns\n",
    "        combined_df = pd.merge(combined_df, batch_df, on=common_columns, how='outer', suffixes=('', '_dup'))\n",
    "        \n",
    "        print(f\"Lote {i // batch_size + 1} combinado exitosamente.\")\n",
    "\n",
    "    # Guarda el archivo final combinado\n",
    "    combined_data_path = \"reporteCombinado/combined_data_final.csv\"\n",
    "    combined_df.to_csv(combined_data_path, index=False)\n",
    "    print(f\"Archivo combinado final guardado en {combined_data_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio del programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontró la sección 'Destacados'.\n",
      "URL del artículo: https://colombiatic.mintic.gov.co/679/w3-article-397520.html\n",
      "El archivo 'reporteColombiaTic\\reporte_tic_segundo_trimestre_2024.xlsx' ya existe. No se realizará la descarga.\n"
     ]
    }
   ],
   "source": [
    "# ## Ejecución del Script\n",
    "\n",
    "# Descargar y procesar el reporte\n",
    "file_path = fetch_report(BASE_URL)  # Descarga y guarda el archivo en subcarpeta /reporteColombiaTic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reporteColombiaTic\\\\reporte_tic_segundo_trimestre_2024.xlsx'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitiendo la hoja 'PORTADA' debido a longitud de columnas insuficiente.\n",
      "Hoja 'CONTENIDO' procesada y guardada en reporteCombinado/temp_lotes\\CONTENIDO_processed.csv\n",
      "Hoja '1' procesada y guardada en reporteCombinado/temp_lotes\\1_processed.csv\n",
      "Hoja '2' procesada y guardada en reporteCombinado/temp_lotes\\2_processed.csv\n",
      "Hoja '3' procesada y guardada en reporteCombinado/temp_lotes\\3_processed.csv\n",
      "Hoja '4,1' procesada y guardada en reporteCombinado/temp_lotes\\4,1_processed.csv\n",
      "Hoja '4,2' procesada y guardada en reporteCombinado/temp_lotes\\4,2_processed.csv\n",
      "Hoja '4,3' procesada y guardada en reporteCombinado/temp_lotes\\4,3_processed.csv\n",
      "Hoja '4,4' procesada y guardada en reporteCombinado/temp_lotes\\4,4_processed.csv\n",
      "Hoja '5' procesada y guardada en reporteCombinado/temp_lotes\\5_processed.csv\n",
      "Hoja '6' procesada y guardada en reporteCombinado/temp_lotes\\6_processed.csv\n",
      "Hoja '7' procesada y guardada en reporteCombinado/temp_lotes\\7_processed.csv\n",
      "Hoja '8' procesada y guardada en reporteCombinado/temp_lotes\\8_processed.csv\n",
      "Hoja '9' procesada y guardada en reporteCombinado/temp_lotes\\9_processed.csv\n",
      "Hoja '10' procesada y guardada en reporteCombinado/temp_lotes\\10_processed.csv\n",
      "Hoja '11' procesada y guardada en reporteCombinado/temp_lotes\\11_processed.csv\n",
      "Hoja '12' procesada y guardada en reporteCombinado/temp_lotes\\12_processed.csv\n",
      "Hoja '13' procesada y guardada en reporteCombinado/temp_lotes\\13_processed.csv\n",
      "Hoja '14' procesada y guardada en reporteCombinado/temp_lotes\\14_processed.csv\n",
      "Omitiendo la hoja '15' debido a la longitud de columnas no coincidente.\n",
      "Hoja '16' procesada y guardada en reporteCombinado/temp_lotes\\16_processed.csv\n",
      "Hoja '17' procesada y guardada en reporteCombinado/temp_lotes\\17_processed.csv\n",
      "Omitiendo la hoja '18' debido a longitud de columnas insuficiente.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS',\n       'TRÁFICO (MB)', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO',\n       'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS',\n       'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO',\n       'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES',\n       'Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n       'Unnamed: 14', 'Unnamed: 15', 'CÓDIGO DANE DEPARTAMENTO',\n       'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO',\n       'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS',\n       'INGRESOS TELEFONIA FIJA', 'No. ACCESOS FIJOS A INTERNET',\n       'CÓDIGO DANE', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n      dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Procesa y combina las hojas del reporte\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Guardar el DataFrame combinado en formato CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#combined_data_path_csv = \"reporteCombinado/combined_data_final.csv\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#combined_df.to_csv(combined_data_path_csv, index=False)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#combined_data_path = \"reporteCombinado\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#save_large_df_to_csv(combined_df, combined_data_path)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 477\u001b[0m, in \u001b[0;36mprocess_report\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# Realiza la combinación por lotes\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     common_columns \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mintersection(batch_df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m batch_df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m--> 477\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_dup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m combinado exitosamente.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Guarda el archivo final combinado\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:142\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 142\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:731\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    727\u001b[0m (\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m--> 731\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1189\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m rk \u001b[38;5;241m=\u001b[39m cast(Hashable, rk)\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(right\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:1778\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1776\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS',\n       'TRÁFICO (MB)', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO',\n       'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS',\n       'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO',\n       'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES',\n       'Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n       'Unnamed: 14', 'Unnamed: 15', 'CÓDIGO DANE DEPARTAMENTO',\n       'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO',\n       'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS',\n       'INGRESOS TELEFONIA FIJA', 'No. ACCESOS FIJOS A INTERNET',\n       'CÓDIGO DANE', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "combined_df = process_report(file_path)  # Procesa y combina las hojas del reporte\n",
    "\n",
    "# Guardar el DataFrame combinado en formato CSV\n",
    "#combined_data_path_csv = \"reporteCombinado/combined_data_final.csv\"\n",
    "#combined_df.to_csv(combined_data_path_csv, index=False)\n",
    "#print(f\"Archivo combinado final guardado en {combined_data_path_csv}\")\n",
    "\n",
    "#combined_data_path = \"reporteCombinado\"\n",
    "#save_large_df_to_csv(combined_df, combined_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopIteration"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 2246815, 16 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Guardar el DataFrame combinado en un archivo final\u001b[39;00m\n\u001b[0;32m      2\u001b[0m combined_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreporteCombinado/combined_data_final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo combinado final guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:2252\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2241\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2242\u001b[0m     df,\n\u001b[0;32m   2243\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2250\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2251\u001b[0m )\n\u001b[1;32m-> 2252\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Josvaldes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:923\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    921\u001b[0m num_rows, num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;129;01mor\u001b[39;00m num_cols \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols:\n\u001b[1;32m--> 923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sheet is too large! Your sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax sheet size is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    926\u001b[0m     )\n\u001b[0;32m    928\u001b[0m formatted_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_formatted_cells()\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writer, ExcelWriter):\n",
      "\u001b[1;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 2246815, 16 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame combinado en un archivo final\n",
    "combined_data_path = \"reporteCombinado/combined_data_final.xlsx\"\n",
    "combined_df.to_excel(combined_data_path, index=False)\n",
    "print(f\"Archivo combinado final guardado en {combined_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un archivo Excel en la subcarpeta /reporteCombinado\n",
    "save_combined_data(combined_df, combined_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Ejecución de Validación y Análisis\n",
    "\n",
    "# Realizar análisis de columnas clave\n",
    "analyze_columns(combined_df)\n",
    "\n",
    "# Manejar valores nulos\n",
    "combined_df = handle_missing_values(combined_df)\n",
    "\n",
    "# Generar estadísticas descriptivas\n",
    "generate_statistics(combined_df)\n",
    "\n",
    "# Guardar la versión final del DataFrame validado\n",
    "save_final_data(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
