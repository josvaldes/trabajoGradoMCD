{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping pagina Web Colombia TIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Reportes de Colombia TIC\n",
    "\n",
    "Este notebook permite automatizar la descarga de reportes de Colombia TIC, extraer y procesar datos relevantes, \n",
    "y finalmente combinar la información en un solo DataFrame para análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias necesarias para el proyecto\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título de la página de prueba: Inicio\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la URL base para la descarga y el uso de carpetas de almacenamiento.\n",
    "\n",
    "BASE_URL = \"https://colombiatic.mintic.gov.co/679/w3-channel.html\"\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "combined_data_path = \"reporteCombinado\"\n",
    "\n",
    "# ### Configuración de Selenium WebDriver\n",
    "\n",
    "# Configurar el servicio de Chrome con la ruta completa\n",
    "service = Service(\"C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe\")\n",
    "\n",
    "# Opciones de Chrome (opcional, para correr en modo headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(BASE_URL)\n",
    "    print(\"Título de la página de prueba:\", driver.title)\n",
    "except Exception as e:\n",
    "    print(\"Error al inicializar ChromeDriver:\", e)\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Función para crear subcarpetas\n",
    "\n",
    "def create_directories(base_path, year, trimester):\n",
    "    \"\"\"\n",
    "    Crea directorios para almacenar los archivos según año y trimestre.\n",
    "    \"\"\"\n",
    "    path = os.path.join(base_path, f\"Año_{year}\", f\"Trimestre_{trimester}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ### Función para descargar el reporte\n",
    "\n",
    "def download_report(url, save_path):\n",
    "    \"\"\"\n",
    "    Descarga el archivo Excel desde la URL proporcionada y lo guarda en el path especificado.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(save_path, \"reporte_tic_trimestral.xlsx\")\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Reporte descargado en {filename}\")\n",
    "    return filename\n",
    "\n",
    "# ## Descarga y Almacenamiento del Reporte\n",
    "\n",
    "def archivo_trimestral_existente(nombre_archivo):\n",
    "    return os.path.exists(nombre_archivo)\n",
    "\n",
    "# Función principal para descargar el reporte\n",
    "# Definir la ruta de la carpeta donde se guardará el archivo descargado\n",
    "download_base_path = \"reporteColombiaTic\"\n",
    "\n",
    "def fetch_report(url):\n",
    "    \"\"\"\n",
    "    Navega en la página de Colombia TIC, encuentra y descarga el archivo Excel más reciente.\n",
    "    \"\"\"\n",
    "    # Configurar el servicio de Chrome\n",
    "    service = Service('C:/Users/Josvaldes/Documents/Maestria/Austral/trabajoGrado/trabajoGradoMCD/Codigo/chromedriver.exe')\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Esperar y encontrar la sección \"Destacados\"\n",
    "        destacados_header = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h2[@class='ntg-titulo-caja' and text()='Destacados']\"))\n",
    "        )\n",
    "        print(\"Se encontró la sección 'Destacados'.\")\n",
    "\n",
    "        # Buscar el enlace al artículo en la sección \"Destacados\"\n",
    "        destacados_section = driver.find_element(By.XPATH, \"//div[@class='recuadro col-md-6 col-lg-4 text-center mb-4']\")\n",
    "        link_element = destacados_section.find_element(By.TAG_NAME, \"a\")\n",
    "        report_url = link_element.get_attribute('href')\n",
    "\n",
    "        print(\"URL del artículo:\", report_url)\n",
    "\n",
    "        # Extraer el trimestre y año del reporte a partir del texto del enlace\n",
    "        trimestre_texto = link_element.text\n",
    "        trimestre = re.search(r\"(primer|segundo|tercer|cuarto) trimestre de (\\d{4})\", trimestre_texto, re.IGNORECASE)\n",
    "        if trimestre:\n",
    "            trimestre_nombre = trimestre.group(1)\n",
    "            año = trimestre.group(2)\n",
    "            nombre_archivo = f\"reporte_tic_{trimestre_nombre}_trimestre_{año}.xlsx\"\n",
    "        else:\n",
    "            print(\"No se pudo determinar el trimestre del reporte.\")\n",
    "            nombre_archivo = \"reporte_tic_trimestral.xlsx\"\n",
    "\n",
    "        # Crear la carpeta de descarga si no existe\n",
    "        os.makedirs(download_base_path, exist_ok=True)\n",
    "        \n",
    "        # Ruta completa donde se guardará el archivo\n",
    "        archivo_path = os.path.join(download_base_path, nombre_archivo)\n",
    "        \n",
    "        # Verificar si el archivo del trimestre ya existe\n",
    "        if os.path.exists(archivo_path):\n",
    "            print(f\"El archivo '{archivo_path}' ya existe. No se realizará la descarga.\")\n",
    "            return archivo_path\n",
    "        else:\n",
    "            # Navegar a la página del artículo\n",
    "            driver.get(report_url)\n",
    "\n",
    "            # Esperar a que el `<span>` que contiene el enlace al archivo Excel esté presente\n",
    "            excel_span = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//span[contains(@class, 'bajardoc') and contains(@class, 'binary-archivo_xls') and contains(@class, 'format-xlsx')]\"))\n",
    "            )\n",
    "\n",
    "            # Obtener el enlace al archivo Excel\n",
    "            excel_link = excel_span.find_element(By.TAG_NAME, \"a\")\n",
    "            excel_url = excel_link.get_attribute('href')\n",
    "            print(\"URL del archivo Excel:\", excel_url)\n",
    "\n",
    "            # Descargar el archivo Excel usando requests\n",
    "            excel_response = requests.get(excel_url)\n",
    "\n",
    "            # Verificar el código de estado antes de guardar\n",
    "            if excel_response.status_code == 200:\n",
    "                with open(archivo_path, 'wb') as file:\n",
    "                    file.write(excel_response.content)\n",
    "                print(f\"Reporte descargado exitosamente como {archivo_path}\")\n",
    "                return archivo_path\n",
    "            else:\n",
    "                print(f\"Error al descargar el archivo. Código de estado: {excel_response.status_code}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"No se encontró la sección 'Destacados' o hubo un error:\", str(e))\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Función para renombrar columnas duplicadas con sufijos secuenciales\n",
    "def rename_duplicates(columns):\n",
    "    \"\"\"\n",
    "    Renombra columnas duplicadas añadiendo un sufijo '_dupN' para evitar conflictos de nombres.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "def save_large_df_to_excel(df, file_path, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en un archivo Excel, dividiéndolo en múltiples hojas si es necesario.\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame a guardar.\n",
    "        file_path (str): Ruta del archivo Excel de salida.\n",
    "        chunk_size (int): Número máximo de filas por hoja.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            df_chunk = df.iloc[i:i + chunk_size]\n",
    "            sheet_name = f\"Sheet_{i // chunk_size + 1}\"\n",
    "            df_chunk.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo combinado final guardado en {file_path}\")\n",
    "\n",
    "def save_large_df_to_csv(df, base_path, chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame grande en múltiples archivos CSV en una subcarpeta especificada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df: DataFrame a guardar.\n",
    "    - base_path: Ruta base para guardar los archivos CSV.\n",
    "    - chunk_size: Tamaño del lote en número de filas para dividir el archivo CSV si es grande.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que la carpeta base exista\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Dividir y guardar el DataFrame en archivos CSV en chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "        file_path = os.path.join(base_path, f\"combined_data_part_{i // chunk_size + 1}.csv\")\n",
    "        chunk.to_csv(file_path, index=False)\n",
    "        print(f\"Guardado el archivo {file_path}\")\n",
    "\n",
    "# ## Procesamiento del Reporte\n",
    "\n",
    "def process_report2(file_path):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del archivo descargado, aplica limpieza y estandarización de columnas.\n",
    "    \"\"\"\n",
    "    # Definir las columnas esperadas para cada hoja\n",
    "    column_structure = {\n",
    "        '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "        '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "        '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "        '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "        '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "        '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "        '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "        '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "        '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "        '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "        '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "        '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "        '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "        '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "    }\n",
    "\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    processed_sheets = {}\n",
    "    \n",
    "    # Función para renombrar columnas duplicadas\n",
    "    def rename_duplicates(columns):\n",
    "        counts = {}\n",
    "        new_columns = []\n",
    "        for col in columns:\n",
    "            if col in counts:\n",
    "                counts[col] += 1\n",
    "                new_columns.append(f\"{col}_dup{counts[col]}\")\n",
    "            else:\n",
    "                counts[col] = 0\n",
    "                new_columns.append(col)\n",
    "        return new_columns\n",
    "\n",
    "    # Limpiar cada hoja y ajustar las columnas\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        if sheet_name not in column_structure:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' (sin estructura conocida)\")\n",
    "            continue\n",
    "\n",
    "        # Limpiar encabezados y resetear índices\n",
    "        df = df.rename(columns=lambda x: x.strip()).dropna(how='all')\n",
    "        \n",
    "        # Renombrar columnas duplicadas\n",
    "        df.columns = rename_duplicates(df.columns)\n",
    "\n",
    "        # Validar si el número de columnas coincide\n",
    "        if len(df.columns) == len(column_structure[sheet_name]):\n",
    "            df.columns = column_structure[sheet_name]\n",
    "            processed_sheets[sheet_name] = df\n",
    "        else:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' debido a la longitud de columnas no coincidente.\")\n",
    "    \n",
    "    # Guardar un archivo temporal para depuración\n",
    "    temp_output_path = os.path.join(\"reporteCombinado\", \"temp_combined_debug.xlsx\")\n",
    "    with pd.ExcelWriter(temp_output_path) as writer:\n",
    "        for sheet_name, df in processed_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Archivo temporal guardado en: {temp_output_path}\")\n",
    "\n",
    "    # Combinar todas las hojas en un solo DataFrame\n",
    "    combined_df = processed_sheets.pop('1')\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        # Verificar y renombrar columnas duplicadas que podrían causar problemas en el merge\n",
    "        for col in df.columns:\n",
    "            if col in combined_df.columns and col != 'AÑO' and col != 'TRIMESTRE':\n",
    "                df = df.rename(columns={col: f\"{col}_{sheet_name}\"})\n",
    "\n",
    "        # Obtener columnas comunes\n",
    "        common_columns = combined_df.columns.intersection(df.columns).tolist()\n",
    "        combined_df = pd.merge(combined_df, df, on=common_columns, how='outer', suffixes=('', f'_{sheet_name}'))\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def process_report3(file_path, temp_folder=\"reporteCombinado/temp_lotes\", min_column_count=5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja de un archivo Excel, la guarda en archivos temporales por lotes y luego combina los resultados.\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo Excel a procesar.\n",
    "        temp_folder (str): Carpeta para guardar los archivos procesados temporalmente.\n",
    "        min_column_count (int): Número mínimo de columnas para procesar una hoja.\n",
    "        batch_size (int): Número de archivos en cada lote para la combinación por lotes.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame combinado de todas las hojas procesadas.\n",
    "    \"\"\"\n",
    "    # Crear la carpeta temporal para guardar archivos procesados\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "    # Cargar el archivo Excel\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Paso 1: Procesa cada hoja y guarda el resultado individualmente\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        try:\n",
    "            # Leer la hoja\n",
    "            df = excel_file.parse(sheet_name)\n",
    "            \n",
    "            # Verificar la estructura de la hoja antes de procesar\n",
    "            if len(df.columns) < min_column_count:\n",
    "                print(f\"Omitiendo la hoja '{sheet_name}' debido a longitud de columnas insuficiente.\")\n",
    "                continue\n",
    "            \n",
    "            # Limpiar los nombres de las columnas y renombrar duplicados\n",
    "            df.columns = [col.strip() for col in df.columns]  # Quitar espacios en blanco\n",
    "            df.columns = rename_duplicates(df.columns)\n",
    "            \n",
    "            # Guardar cada hoja procesada en un archivo temporal CSV\n",
    "            temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "            df.to_csv(temp_file_path, index=False)\n",
    "            print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "\n",
    "    # Paso 2: Combina los archivos temporales en lotes\n",
    "    # Lista de archivos temporales procesados\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    # DataFrame combinado final\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Combinación por lotes\n",
    "    for i in range(0, len(temp_files), batch_size):\n",
    "        batch_files = temp_files[i:i + batch_size]\n",
    "        \n",
    "        # Leer y combinar el lote actual\n",
    "        batch_df = pd.concat([pd.read_csv(f) for f in batch_files], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Combina el lote con el DataFrame final\n",
    "        if not combined_df.empty:\n",
    "            common_columns = combined_df.columns.intersection(batch_df.columns).tolist()\n",
    "            combined_df = pd.merge(combined_df, batch_df, on=common_columns, how='outer', suffixes=('', '_dup'))\n",
    "        else:\n",
    "            combined_df = batch_df.copy()  # Inicializar con el primer lote\n",
    "        \n",
    "        print(f\"Lote {i // batch_size + 1} combinado exitosamente.\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# ## Guardar el DataFrame Combinado\n",
    "\n",
    "def save_combined_data(df, path):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame combinado en un archivo Excel en la carpeta 'reporteCombinado'.\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(path, \"data_reporte_combined_final.xlsx\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"DataFrame combinado guardado en: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ## Análisis y Validación de Datos\n",
    "\n",
    "# ### Función para verificar valores únicos y posibles inconsistencias\n",
    "\n",
    "def analyze_columns(df):\n",
    "    \"\"\"\n",
    "    Muestra valores únicos y verifica consistencias en las columnas clave.\n",
    "    \"\"\"\n",
    "    columnas_clave = ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET']\n",
    "    \n",
    "    # Mostrar valores únicos en columnas clave\n",
    "    for col in columnas_clave:\n",
    "        if col in df.columns:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"\\nValores únicos en '{col}':\", unique_values[:10])  # Muestra los primeros 10 valores únicos\n",
    "\n",
    "# ### Función para identificar y manejar datos nulos\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identifica y maneja valores nulos en el DataFrame, aplicando reemplazos si es necesario.\n",
    "    \"\"\"\n",
    "    missing_info = df.isnull().sum()\n",
    "    print(\"\\nValores nulos en cada columna:\\n\", missing_info)\n",
    "    \n",
    "    # Puedes aplicar reemplazos según lo necesario, por ejemplo, reemplazar NaN en algunas columnas\n",
    "    df['No. ACCESOS FIJOS A INTERNET'] = df['No. ACCESOS FIJOS A INTERNET'].fillna(0)\n",
    "    # Más reemplazos pueden añadirse aquí según los análisis\n",
    "    return df\n",
    "\n",
    "# ### Función para generar estadísticas descriptivas\n",
    "\n",
    "def generate_statistics(df):\n",
    "    \"\"\"\n",
    "    Genera estadísticas descriptivas para las columnas numéricas del DataFrame.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    print(\"\\nEstadísticas descriptivas de columnas numéricas:\\n\", df[numeric_cols].describe())\n",
    "\n",
    "# ### Guardar versión final del DataFrame para análisis\n",
    "\n",
    "def save_final_data(df):\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame limpio y validado en un archivo Excel en la subcarpeta /reporteCombinado para análisis final.\n",
    "    \"\"\"\n",
    "    final_output_path = os.path.join(combined_data_path, \"data_reporte_combined_validated.xlsx\")\n",
    "    df.to_excel(final_output_path, index=False)\n",
    "    print(f\"DataFrame final validado guardado en: {final_output_path}\")\n",
    "\n",
    "\n",
    "# Carpeta para los archivos procesados temporalmente\n",
    "temp_folder = \"reporteCombinado/temp_lotes\"\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "# Mínimo número de columnas esperadas para cada hoja (ajustar según tu estructura)\n",
    "cierto_numero_de_columnas_minimo = 4\n",
    "\n",
    "# Estructura esperada de columnas por hoja (añadir según tu estructura)\n",
    "column_structure = {\n",
    "    '1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '2': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '3': ['AÑO', 'TRIMESTRE', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'No. ACCESOS FIJOS A INTERNET', 'POBLACIÓN DANE', 'PENETRACIÓN'],\n",
    "    '4,1': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,2': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,3': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '4,4': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'SEGMENTO', 'TECNOLOGÍA', 'VELOCIDAD BAJADA', 'VELOCIDAD SUBIDA', 'No. ACCESOS FIJOS A INTERNET'],\n",
    "    '5': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS (Pesos)'],\n",
    "    '6': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. ABONADOS'],\n",
    "    '7': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '8': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TRÁFICO (MB)'],\n",
    "    '9': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'TECNOLOGÍA', 'No. SUSCRIPTORES'],\n",
    "    '10': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'SEGMENTO', 'TERMINAL', 'INGRESOS'],\n",
    "    '11': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'TRÁFICO (MB)'],\n",
    "    '12': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'LÍNEAS EN SERVICIO', 'LÍNEAS PREPAGO', 'LÍNEAS POSPAGO', 'LÍNEAS ACTIVADAS', 'LÍNEAS RETIRADAS'],\n",
    "    '13': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'PROVEEDOR DESTINO', 'TRÁFICO PREPAGO', 'TRÁFICO POSPAGO'],\n",
    "    '14': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CONSUMO PREPAGO', 'CONSUMO POSPAGO', 'INGRESOS OPERACIONALES'],\n",
    "    '15': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE', 'DEPARTAMENTO', 'CÓDIGO DANE', 'MUNICIPIO', 'CABECERA MUNICIPAL', 'CÓDIGO DANE', 'CENTRO POBLADO', 'COBERTURA 2G', 'COBERTURA 3G', 'COBERTURA HSPA+', 'HSPA+DC', 'COBERTURA_4G', 'COBERTURA_LTE', 'COBERTURA_5G'],\n",
    "    '16': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'CÓDIGO DANE DEPARTAMENTO', 'DEPARTAMENTO', 'CÓDIGO DANE MUNICIPIO', 'MUNICIPIO', 'SEGMENTO', 'LÍNEAS EN SERVICIO'],\n",
    "    '17': ['AÑO', 'TRIMESTRE', 'PROVEEDOR', 'INGRESOS TRÁFICO LDIE', 'INGRESOS TRÁFICO LDIS', 'INGRESOS TELEFONIA FIJA']\n",
    "}\n",
    "# Función para limpiar y estandarizar cada hoja\n",
    "# Función para limpiar y estandarizar cada hoja\n",
    "def limpiar_hoja(df, columnas_esperadas):\n",
    "    \"\"\"\n",
    "    Limpia y estandariza cada hoja para alinear columnas.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='all')  # Elimina filas completamente vacías\n",
    "    df.columns = df.columns.str.strip()  # Limpia espacios en nombres de columnas\n",
    "    valid_row_index = df[df.iloc[:, 0].astype(str).str.match(r'^\\d{4}$')].index.min()\n",
    "    \n",
    "    # Identificar y procesar desde la primera fila válida\n",
    "    if valid_row_index is not None:\n",
    "        df = df.iloc[valid_row_index:]\n",
    "        df.columns = columnas_esperadas[:len(df.columns)]  # Ajusta las columnas según el número de columnas reales\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Rellena columnas faltantes para que coincida con columnas_esperadas\n",
    "        df = df.reindex(columns=columnas_esperadas, fill_value=\"\")\n",
    "    else:\n",
    "        print(\"No se encontraron filas válidas en la hoja.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Detectar todas las columnas únicas en el archivo\n",
    "def detect_all_columns(file_path, column_structure):\n",
    "    \"\"\"\n",
    "    Detecta todas las columnas únicas en el archivo para usarlas en la combinación, según la estructura definida.\n",
    "    \"\"\"\n",
    "    all_columns = set()\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        if sheet_name in column_structure:\n",
    "            all_columns.update(column_structure[sheet_name])\n",
    "    return list(all_columns)\n",
    "\n",
    "# Procesar y combinar reporte\n",
    "def process_report(file_path, column_structure, temp_folder=\"reporteCombinado/temp_lotes\"):\n",
    "    \"\"\"\n",
    "    Procesa cada hoja del reporte, limpia y alinea columnas, y combina el resultado en un archivo CSV final.\n",
    "    \"\"\"\n",
    "    # Detectar todas las columnas únicas basadas en column_structure\n",
    "    all_columns = detect_all_columns(file_path, column_structure)\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    # Procesamiento y almacenamiento de cada hoja\n",
    "    for sheet_name in pd.ExcelFile(file_path).sheet_names:\n",
    "        if sheet_name in column_structure:\n",
    "            try:\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                cleaned_df = limpiar_hoja(df, column_structure[sheet_name])\n",
    "                \n",
    "                # Guardar cada hoja procesada en un archivo temporal CSV\n",
    "                temp_file_path = os.path.join(temp_folder, f\"{sheet_name}_processed.csv\")\n",
    "                cleaned_df.to_csv(temp_file_path, index=False)\n",
    "                print(f\"Hoja '{sheet_name}' procesada y guardada en {temp_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar la hoja '{sheet_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Omitiendo la hoja '{sheet_name}' (estructura desconocida)\")\n",
    "\n",
    "    # Combinación de los archivos CSV temporales en un archivo final\n",
    "    combined_data_path = \"reporteCombinado/combined_data_final.csv\"\n",
    "    temp_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.endswith('_processed.csv')]\n",
    "\n",
    "    with open(combined_data_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        for i, temp_file in enumerate(temp_files):\n",
    "            batch_df = pd.read_csv(temp_file, dtype=str, low_memory=False)\n",
    "            batch_df = batch_df.reindex(columns=all_columns, fill_value=\"\")  # Alinear todas las columnas\n",
    "            \n",
    "            # Concatenar archivos con encabezado solo en el primer archivo\n",
    "            batch_df.to_csv(output_file, mode='a', header=(i == 0), index=False)\n",
    "            print(f\"Archivo {temp_file} concatenado exitosamente en {combined_data_path}\")\n",
    "\n",
    "    print(f\"Archivo combinado final guardado en {combined_data_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio del programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontró la sección 'Destacados'.\n",
      "URL del artículo: https://colombiatic.mintic.gov.co/679/w3-article-397520.html\n",
      "El archivo 'reporteColombiaTic\\reporte_tic_segundo_trimestre_2024.xlsx' ya existe. No se realizará la descarga.\n"
     ]
    }
   ],
   "source": [
    "# ## Ejecución del Script\n",
    "\n",
    "# Descargar y procesar el reporte\n",
    "file_path = fetch_report(BASE_URL)  # Descarga y guarda el archivo en subcarpeta /reporteColombiaTic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitiendo la hoja 'PORTADA' (estructura desconocida)\n",
      "Omitiendo la hoja 'CONTENIDO' (estructura desconocida)\n",
      "Hoja '1' procesada y guardada en reporteCombinado/temp_lotes\\1_processed.csv\n",
      "Hoja '2' procesada y guardada en reporteCombinado/temp_lotes\\2_processed.csv\n",
      "Hoja '3' procesada y guardada en reporteCombinado/temp_lotes\\3_processed.csv\n",
      "Hoja '4,1' procesada y guardada en reporteCombinado/temp_lotes\\4,1_processed.csv\n",
      "Hoja '4,2' procesada y guardada en reporteCombinado/temp_lotes\\4,2_processed.csv\n",
      "Hoja '4,3' procesada y guardada en reporteCombinado/temp_lotes\\4,3_processed.csv\n",
      "Hoja '4,4' procesada y guardada en reporteCombinado/temp_lotes\\4,4_processed.csv\n",
      "Hoja '5' procesada y guardada en reporteCombinado/temp_lotes\\5_processed.csv\n",
      "Hoja '6' procesada y guardada en reporteCombinado/temp_lotes\\6_processed.csv\n",
      "Hoja '7' procesada y guardada en reporteCombinado/temp_lotes\\7_processed.csv\n",
      "Hoja '8' procesada y guardada en reporteCombinado/temp_lotes\\8_processed.csv\n",
      "Hoja '9' procesada y guardada en reporteCombinado/temp_lotes\\9_processed.csv\n",
      "Hoja '10' procesada y guardada en reporteCombinado/temp_lotes\\10_processed.csv\n",
      "Hoja '11' procesada y guardada en reporteCombinado/temp_lotes\\11_processed.csv\n",
      "Hoja '12' procesada y guardada en reporteCombinado/temp_lotes\\12_processed.csv\n",
      "Hoja '13' procesada y guardada en reporteCombinado/temp_lotes\\13_processed.csv\n",
      "Hoja '14' procesada y guardada en reporteCombinado/temp_lotes\\14_processed.csv\n",
      "Error al procesar la hoja '15': cannot reindex on an axis with duplicate labels\n",
      "Hoja '16' procesada y guardada en reporteCombinado/temp_lotes\\16_processed.csv\n",
      "Hoja '17' procesada y guardada en reporteCombinado/temp_lotes\\17_processed.csv\n",
      "Omitiendo la hoja '18' (estructura desconocida)\n",
      "Archivo reporteCombinado/temp_lotes\\10_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\11_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\12_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\13_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\14_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\15_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\16_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\17_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\1_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\2_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\3_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\4,1_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\4,2_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\4,3_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\4,4_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\5_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\6_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\7_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\8_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\9_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo reporteCombinado/temp_lotes\\CONTENIDO_processed.csv concatenado exitosamente en reporteCombinado/combined_data_final.csv\n",
      "Archivo combinado final guardado en reporteCombinado/combined_data_final.csv\n"
     ]
    }
   ],
   "source": [
    "combined_df = process_report(file_path, column_structure)  # Procesa y combina las hojas del reporte\n",
    "\n",
    "# Guardar el DataFrame combinado en formato CSV\n",
    "#combined_data_path_csv = \"reporteCombinado/combined_data_final.csv\"\n",
    "#combined_df.to_csv(combined_data_path_csv, index=False)\n",
    "#print(f\"Archivo combinado final guardado en {combined_data_path_csv}\")\n",
    "\n",
    "#combined_data_path = \"reporteCombinado\"\n",
    "#save_large_df_to_csv(combined_df, combined_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopIteration"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Guardar el DataFrame combinado en un archivo final\u001b[39;00m\n\u001b[0;32m      2\u001b[0m combined_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreporteCombinado/combined_data_final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m(combined_data_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo combinado final guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_excel'"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame combinado en un archivo final\n",
    "combined_data_path = \"reporteCombinado/combined_data_final.xlsx\"\n",
    "combined_df.to_excel(combined_data_path, index=False)\n",
    "print(f\"Archivo combinado final guardado en {combined_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un archivo Excel en la subcarpeta /reporteCombinado\n",
    "save_combined_data(combined_df, combined_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Ejecución de Validación y Análisis\n",
    "\n",
    "# Realizar análisis de columnas clave\n",
    "analyze_columns(combined_df)\n",
    "\n",
    "# Manejar valores nulos\n",
    "combined_df = handle_missing_values(combined_df)\n",
    "\n",
    "# Generar estadísticas descriptivas\n",
    "generate_statistics(combined_df)\n",
    "\n",
    "# Guardar la versión final del DataFrame validado\n",
    "save_final_data(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
