{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Sygigwyy6b53zxHs3fR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josvaldes/trabajoGradoMCD/blob/develop/scrapingColombiaTicFinal3_ipynb_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Notebook 1 - ColombiaTIC: Scraping + Carga Incremental a DuckDB\n",
        "#  Versi√≥n optimizada: conexi√≥n segura, control_cargue, y logs persistentes\n",
        "# ============================================================\n",
        "\n",
        "# ========== 0. Preparaci√≥n e instalaciones (auto) ==========\n",
        "import sys, subprocess, importlib\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            importlib.import_module(p if p != 'requests_html' else 'requests_html')\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Instalando {p}...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", p, \"-q\"], check=True)\n",
        "\n",
        "# Paquetes requeridos\n",
        "pip_install([\"duckdb\", \"pandas\", \"openpyxl\", \"tqdm\", \"requests\", \"requests_html\", \"lxml_html_clean\", \"nest_asyncio\"])\n",
        "\n",
        "# ========== 1. Imports y paths ==========\n",
        "import os, re, shutil, time, logging\n",
        "from datetime import datetime\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from requests_html import AsyncHTMLSession\n",
        "from google.colab import drive\n",
        "\n",
        "# Rutas base (ajusta si lo necesitas)\n",
        "RUTA_EXCEL_DEST   = \"/content/gdrive/MyDrive/trabajoGrado/reporte_colombiatic\"\n",
        "RUTA_EXCEL_TEMP   = \"/content/gdrive/MyDrive/trabajoGrado/temp_colombiatic\"\n",
        "RUTA_DB_DRIVE_DIR = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos\"\n",
        "RUTA_DB_DRIVE     = os.path.join(RUTA_DB_DRIVE_DIR, \"colombiatic.duckdb\")\n",
        "RUTA_DB_LOCAL     = \"/content/colombiatic_temp.duckdb\"     # base temporal local para evitar IO de Drive\n",
        "RUTA_LOG          = os.path.join(RUTA_DB_DRIVE_DIR, \"colombiatic_etl.log\")  # LOG persistente junto a la base\n",
        "\n",
        "os.makedirs(RUTA_EXCEL_DEST, exist_ok=True)\n",
        "os.makedirs(RUTA_EXCEL_TEMP, exist_ok=True)\n",
        "os.makedirs(RUTA_DB_DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================\n",
        "# üîó Montaje robusto de Google Drive\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "mount_path = \"/content/gdrive\"\n",
        "\n",
        "try:\n",
        "    # Si ya existe y contiene archivos, lo desmontamos primero\n",
        "    if os.path.exists(mount_path) and os.listdir(mount_path):\n",
        "        print(\"‚öôÔ∏è  Desmontando Drive previo...\")\n",
        "        drive.flush_and_unmount()\n",
        "        shutil.rmtree(mount_path, ignore_errors=True)\n",
        "\n",
        "    # Montar Drive limpio\n",
        "    drive.mount(mount_path, force_remount=True)\n",
        "    print(\"‚úÖ Google Drive montado correctamente en:\", mount_path)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error al montar Google Drive: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# ========== 2. Logger persistente ==========\n",
        "logger = logging.getLogger(\"colombiatic_etl\")\n",
        "logger.setLevel(logging.INFO)\n",
        "# Evitar handlers duplicados en re-ejecuciones\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(RUTA_LOG, encoding=\"utf-8\")\n",
        "    fh.setLevel(logging.INFO)\n",
        "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "    fh.setFormatter(fmt)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "def log_info(msg):\n",
        "    print(msg)\n",
        "    logger.info(msg)\n",
        "\n",
        "def log_warn(msg):\n",
        "    print(msg)\n",
        "    logger.warning(msg)\n",
        "\n",
        "def log_error(msg):\n",
        "    print(msg)\n",
        "    logger.error(msg)\n",
        "\n",
        "log_info(\"===== INICIO EJECUCI√ìN NOTEBOOK 1 (Scraping + ETL) =====\")\n",
        "log_info(f\"Carpeta destino Excel: {RUTA_EXCEL_DEST}\")\n",
        "log_info(f\"Carpeta temporal Excel: {RUTA_EXCEL_TEMP}\")\n",
        "log_info(f\"Base en Drive: {RUTA_DB_DRIVE}\")\n",
        "log_info(f\"Log persistente: {RUTA_LOG}\")\n",
        "\n",
        "# ========== 3. Utilidades (nombres y conexi√≥n) ==========\n",
        "def normalizar_nombre(s: str, max_len: int = 60) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return s[:max_len] if max_len else s\n",
        "\n",
        "def conectar_duckdb_seguro(path_db: str):\n",
        "    \"\"\"Conecta o crea base. Si existe en Drive, copia a local y conecta local.\"\"\"\n",
        "    # Si existe base en Drive, hacer copia local para trabajar r√°pido/sin locks\n",
        "    if os.path.exists(RUTA_DB_DRIVE):\n",
        "        try:\n",
        "            if os.path.exists(RUTA_DB_LOCAL):\n",
        "                os.remove(RUTA_DB_LOCAL)\n",
        "            shutil.copy2(RUTA_DB_DRIVE, RUTA_DB_LOCAL)\n",
        "            log_info(f\"‚úÖ Copia local creada desde Drive: {RUTA_DB_DRIVE} ‚Üí {RUTA_DB_LOCAL}\")\n",
        "        except Exception as e:\n",
        "            log_warn(f\"‚ö†Ô∏è No se pudo copiar base desde Drive. Se crear√° una nueva local. Detalle: {e}\")\n",
        "    # Conectar a la copia local (o nueva si no exist√≠a)\n",
        "    con = duckdb.connect(RUTA_DB_LOCAL)\n",
        "    log_info(f\"üíæ Conexi√≥n establecida con base local: {RUTA_DB_LOCAL}\")\n",
        "    # Tabla de control (si no existe)\n",
        "    con.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS control_cargue (\n",
        "        archivo TEXT,\n",
        "        hojas_cargadas INTEGER,\n",
        "        filas_totales INTEGER,\n",
        "        fecha_cargue TIMESTAMP,\n",
        "        estado TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "    log_info(\"üìä Tabla 'control_cargue' lista.\")\n",
        "    return con\n",
        "\n",
        "def sincronizar_local_a_drive():\n",
        "    \"\"\"Cierra conexi√≥n si est√° abierta y copia local ‚Üí Drive.\"\"\"\n",
        "    try:\n",
        "        con.close()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if os.path.exists(RUTA_DB_LOCAL):\n",
        "            shutil.copy2(RUTA_DB_LOCAL, RUTA_DB_DRIVE)\n",
        "            log_info(f\"üì¶ Base actualizada copiada de nuevo al Drive: {RUTA_DB_LOCAL} ‚Üí {RUTA_DB_DRIVE}\")\n",
        "    except Exception as e:\n",
        "        log_error(f\"‚ùå Error copiando base a Drive: {e}\")\n",
        "\n",
        "# ========== 4. Scraping (encuentra y descarga solo Excel; filtra TIC) ==========\n",
        "URL_PORTAL = \"https://colombiatic.mintic.gov.co/679/w3-channel.html\"\n",
        "\n",
        "async def obtener_urls_excel():\n",
        "    \"\"\"Rastrea art√≠culos del canal y devuelve URLs .xlsx √∫nicas.\"\"\"\n",
        "    session = AsyncHTMLSession()\n",
        "    try:\n",
        "        r = await session.get(URL_PORTAL)\n",
        "        await r.html.arender(timeout=60, sleep=2)\n",
        "        # enlances a art√≠culos del canal\n",
        "        links = r.html.absolute_links\n",
        "        urls_articulos = [u for u in links if \"/w3-article-\" in u or \"/articles-\" in u]\n",
        "        urls_articulos = list(dict.fromkeys(urls_articulos))  # √∫nicos\n",
        "        log_info(f\"üì∞ Art√≠culos detectados: {len(urls_articulos)}\")\n",
        "\n",
        "        urls_xlsx = set()\n",
        "        for u in tqdm(urls_articulos, desc=\"üîé Explorando art√≠culos\"):\n",
        "            try:\n",
        "                a = await session.get(u)\n",
        "                await a.html.arender(timeout=60, sleep=1)\n",
        "                for l in a.html.absolute_links:\n",
        "                    if l.lower().endswith(\".xlsx\") or \"archivo_xls.xlsx\" in l.lower():\n",
        "                        urls_xlsx.add(l)\n",
        "            except Exception as e:\n",
        "                log_warn(f\"‚ö†Ô∏è Error explorando {u}: {e}\")\n",
        "\n",
        "        return list(urls_xlsx)\n",
        "    finally:\n",
        "        await session.close()\n",
        "\n",
        "def descargar_excel(url: str, destino_dir: str) -> str | None:\n",
        "    \"\"\"Descarga un Excel y lo nombra a partir del slug del art√≠culo o filename; retorna ruta destino.\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=60)\n",
        "        if resp.status_code != 200:\n",
        "            log_warn(f\"‚ö†Ô∏è No se pudo descargar {url} (status {resp.status_code})\")\n",
        "            return None\n",
        "        # nombre base por defecto\n",
        "        base = url.split(\"/\")[-1].replace(\".xlsx\", \"\")\n",
        "        base = normalizar_nombre(base, 80)\n",
        "\n",
        "        # guardar en temp\n",
        "        nombre = f\"{base}.xlsx\"\n",
        "        ruta = os.path.join(destino_dir, nombre)\n",
        "        with open(ruta, \"wb\") as f:\n",
        "            f.write(resp.content)\n",
        "        return ruta\n",
        "    except Exception as e:\n",
        "        log_warn(f\"‚ö†Ô∏è Error descargando {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Lanzar scraping\n",
        "log_info(\"üåê Iniciando scraping de boletines...\")\n",
        "try:\n",
        "    # Ejecutar asincr√≥nicamente en Colab\n",
        "    import asyncio\n",
        "    urls_xlsx = asyncio.get_event_loop().run_until_complete(obtener_urls_excel())\n",
        "except RuntimeError:\n",
        "    # Loop ya corriendo: usar nest_asyncio y crear uno nuevo\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    urls_xlsx = loop.run_until_complete(obtener_urls_excel())\n",
        "\n",
        "log_info(f\"üìä Archivos Excel detectados (√∫nicos): {len(urls_xlsx)}\")\n",
        "\n",
        "# Descarga a carpeta temporal\n",
        "descargados = []\n",
        "for u in tqdm(urls_xlsx, desc=\"‚¨áÔ∏è Descargando archivos Excel\"):\n",
        "    ruta_tmp = descargar_excel(u, RUTA_EXCEL_TEMP)\n",
        "    if ruta_tmp:\n",
        "        descargados.append(ruta_tmp)\n",
        "\n",
        "log_info(f\"üì¶ Archivos descargados en temp: {len(descargados)}\")\n",
        "\n",
        "# Detectar TIC y mover a carpeta final. Borrar los no TIC\n",
        "movidos = []\n",
        "eliminados = []\n",
        "for ruta in descargados:\n",
        "    nombre = os.path.basename(ruta)\n",
        "    # Heur√≠stica: debe contener \"tic\" en el nombre (en muchos casos funciona),\n",
        "    # y/o es el archivo principal que el portal publica (ya trae \"sector_tic\" o similar).\n",
        "    if \"tic\" in nombre.lower():\n",
        "        destino = os.path.join(RUTA_EXCEL_DEST, nombre)\n",
        "        if os.path.exists(destino):\n",
        "            os.remove(destino)\n",
        "        shutil.move(ruta, destino)\n",
        "        movidos.append(destino)\n",
        "        log_info(f\"üì¶ Movido a carpeta destino: {os.path.basename(destino)}\")\n",
        "    else:\n",
        "        os.remove(ruta)\n",
        "        eliminados.append(nombre)\n",
        "        log_info(f\"üßπ Eliminado archivo no relevante: {nombre}\")\n",
        "\n",
        "# Limpieza final de temp (si qued√≥ algo)\n",
        "try:\n",
        "    for f in os.listdir(RUTA_EXCEL_TEMP):\n",
        "        try:\n",
        "            os.remove(os.path.join(RUTA_EXCEL_TEMP, f))\n",
        "        except:\n",
        "            pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "log_info(\"üéØ Scraping finalizado.\")\n",
        "\n",
        "# ========== 5. Conexi√≥n a DuckDB (local) ==========\n",
        "# Importante: conectar despu√©s del scraping\n",
        "con = conectar_duckdb_seguro(RUTA_DB_DRIVE)\n",
        "\n",
        "# ========== 6. Carga incremental de Excel TIC ==========\n",
        "def archivos_tic_actuales():\n",
        "    return [f for f in os.listdir(RUTA_EXCEL_DEST) if f.lower().endswith((\".xlsx\", \".xls\"))]\n",
        "\n",
        "# Obt√©n los ya cargados desde control_cargue\n",
        "try:\n",
        "    ya_cargados = set(\n",
        "        con.execute(\"SELECT DISTINCT archivo FROM control_cargue\").fetchdf()[\"archivo\"].tolist()\n",
        "    )\n",
        "except Exception:\n",
        "    ya_cargados = set()\n",
        "\n",
        "archivos = archivos_tic_actuales()\n",
        "pendientes = [f for f in archivos if f not in ya_cargados]\n",
        "\n",
        "print(\"\\nüìÇ Archivos disponibles actualmente en carpeta destino:\")\n",
        "for f in archivos:\n",
        "    print(f\"   ‚Ä¢ {f}\")\n",
        "\n",
        "if not pendientes:\n",
        "    log_info(\"‚è≠Ô∏è No hay archivos nuevos para cargar. La base est√° actualizada.\")\n",
        "else:\n",
        "    log_info(f\"üÜï Archivos TIC nuevos detectados: {len(pendientes)}\")\n",
        "\n",
        "for nombre_archivo in pendientes:\n",
        "    ruta_archivo = os.path.join(RUTA_EXCEL_DEST, nombre_archivo)\n",
        "    estado = \"OK\"\n",
        "    total_filas = 0\n",
        "    hojas_cargadas = 0\n",
        "    log_info(f\"\\nüìò Procesando: {nombre_archivo}\")\n",
        "\n",
        "    try:\n",
        "        xls = pd.ExcelFile(ruta_archivo)\n",
        "        hojas = xls.sheet_names\n",
        "        # proceso hoja a hoja\n",
        "        for hoja in hojas:\n",
        "            try:\n",
        "                # Detectar encabezado: primeras 10 filas probando\n",
        "                df_valid = None\n",
        "                for fila_enc in range(0, 10):\n",
        "                    df_try = pd.read_excel(ruta_archivo, sheet_name=hoja, header=fila_enc, engine=\"openpyxl\")\n",
        "                    if df_try.columns.notna().sum() > 2:\n",
        "                        df_valid = df_try\n",
        "                        break\n",
        "                if df_valid is None or df_valid.empty:\n",
        "                    continue\n",
        "\n",
        "                df_valid = df_valid.astype(str)\n",
        "\n",
        "                # nombre de tabla\n",
        "                base = normalizar_nombre(os.path.splitext(nombre_archivo)[0], 48)\n",
        "                hoja_n = normalizar_nombre(hoja, 10)  # corto para no pasarse de 63\n",
        "                nombre_tabla = f\"{base}_{hoja_n}\"\n",
        "\n",
        "                # Evitar duplicados de tabla\n",
        "                tablas_existentes = con.execute(\"SHOW TABLES\").fetchdf()[\"name\"].tolist()\n",
        "                if nombre_tabla in tablas_existentes:\n",
        "                    log_info(f\"   ‚è≠Ô∏è La tabla '{nombre_tabla}' ya existe, se omite.\")\n",
        "                else:\n",
        "                    con.register(\"tmp_df\", df_valid)\n",
        "                    con.execute(f\"CREATE TABLE '{nombre_tabla}' AS SELECT * FROM tmp_df\")\n",
        "                    con.unregister(\"tmp_df\")\n",
        "                    hojas_cargadas += 1\n",
        "                    total_filas += len(df_valid)\n",
        "                    log_info(f\"   ‚úÖ Hoja '{hoja}' cargada como '{nombre_tabla}' ({len(df_valid)} filas)\")\n",
        "            except Exception as e_hoja:\n",
        "                estado = f\"ERROR_HOJA:{hoja}:{str(e_hoja)[:150]}\"\n",
        "                log_warn(f\"‚ö†Ô∏è {estado}\")\n",
        "\n",
        "    except Exception as e_file:\n",
        "        estado = f\"ERROR_ARCHIVO:{str(e_file)[:150]}\"\n",
        "        log_warn(f\"‚ö†Ô∏è {estado}\")\n",
        "\n",
        "    # Registrar cargue a nivel archivo (incremental)\n",
        "    try:\n",
        "        con.execute(\"\"\"\n",
        "            INSERT INTO control_cargue (archivo, hojas_cargadas, filas_totales, fecha_cargue, estado)\n",
        "            VALUES (?, ?, ?, ?, ?)\n",
        "        \"\"\", [nombre_archivo, hojas_cargadas, total_filas, datetime.now(), estado])\n",
        "        log_info(f\"üìù Registrado en control_cargue: {nombre_archivo} | hojas={hojas_cargadas} | filas={total_filas} | estado={estado}\")\n",
        "    except Exception as e_ins:\n",
        "        log_warn(f\"‚ö†Ô∏è Error insertando en control_cargue: {e_ins}\")\n",
        "\n",
        "# Sincronizar base local a Drive y cerrar\n",
        "sincronizar_local_a_drive()\n",
        "\n",
        "# ========== 7. Revisar la base (resumen final) ==========\n",
        "# Para evitar el error de conexi√≥n cerrada, abrimos una conexi√≥n **ligera** a la base en Drive solo para consulta\n",
        "try:\n",
        "    con_check = duckdb.connect(RUTA_DB_DRIVE, read_only=True)\n",
        "    tablas = con_check.execute(\"SHOW TABLES\").fetchdf()\n",
        "    print(\"\\nüìÇ Tablas en la base:\")\n",
        "    display(tablas)\n",
        "\n",
        "    control = con_check.execute(\"\"\"\n",
        "        SELECT archivo, hojas_cargadas, filas_totales, fecha_cargue, estado\n",
        "        FROM control_cargue\n",
        "        ORDER BY fecha_cargue DESC\n",
        "        LIMIT 20\n",
        "    \"\"\").fetchdf()\n",
        "    print(\"\\nüìã √öltimos registros en control_cargue:\")\n",
        "    display(control)\n",
        "\n",
        "    total_control = con_check.execute(\"SELECT COUNT(*) AS total FROM control_cargue\").fetchdf()\n",
        "    print(\"\\nüßÆ Total de registros en control_cargue:\")\n",
        "    display(total_control)\n",
        "\n",
        "    # tama√±o del archivo de base\n",
        "    tam_mb = os.path.getsize(RUTA_DB_DRIVE) / (1024*1024)\n",
        "    print(f\"\\nüíæ Tama√±o actual de la base: {tam_mb:.2f} MB\")\n",
        "    con_check.close()\n",
        "except Exception as e:\n",
        "    log_warn(f\"‚ö†Ô∏è No se pudo consultar la base final: {e}\")\n",
        "\n",
        "log_info(\"===== FIN EJECUCI√ìN NOTEBOOK 1 =====\")\n",
        "print(\"\\n‚úÖ Proceso completado.\")\n",
        "print(f\"üóÇ Log del ETL: {RUTA_LOG}\")\n",
        "print(f\"üóÑÔ∏è Base DuckDB: {RUTA_DB_DRIVE}\")\n",
        "print(f\"üìÅ Carpeta Excel (solo TIC): {RUTA_EXCEL_DEST}\")\n"
      ],
      "metadata": {
        "id": "IkPdNm2aSI0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb, pandas as pd\n",
        "\n",
        "# Conexi√≥n (solo lectura)\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path, read_only=True)\n",
        "\n",
        "# Listar todas las tablas que contengan \"_4_1\"\n",
        "tablas_41 = con.execute(\"\"\"\n",
        "    SELECT table_name AS name\n",
        "    FROM duckdb_tables()\n",
        "    WHERE table_name LIKE '%4_1%'\n",
        "    ORDER BY table_name\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"üìã Tablas 4_1 detectadas:\")\n",
        "display(tablas_41)\n",
        "\n"
      ],
      "metadata": {
        "id": "hh-cbXZEZNIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de c√≥digo final para consolidar autom√°ticamente"
      ],
      "metadata": {
        "id": "OihBvyvSc4Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb, pandas as pd\n",
        "\n",
        "# --- Asegurar conexi√≥n limpia ---\n",
        "try:\n",
        "    con.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path)\n",
        "print(\"‚úÖ Conectado a la base correctamente.\\n\")\n",
        "\n",
        "# --- Buscar todas las tablas que terminen en _4_1 ---\n",
        "tablas_41 = con.execute(\"\"\"\n",
        "    SELECT table_name AS name\n",
        "    FROM duckdb_tables()\n",
        "    WHERE table_name LIKE '%4_1'\n",
        "    ORDER BY table_name\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "if tablas_41.empty:\n",
        "    print(\"‚ö†Ô∏è No hay tablas 4.1 disponibles.\")\n",
        "else:\n",
        "    print(f\"üìã Tablas detectadas para consolidar ({len(tablas_41)}):\")\n",
        "    display(tablas_41)\n",
        "\n",
        "    # --- Obtener todas las columnas √∫nicas entre las tablas 4.1 ---\n",
        "    columnas_union = set()\n",
        "    columnas_por_tabla = {}\n",
        "\n",
        "    for t in tablas_41[\"name\"]:\n",
        "        cols = con.execute(f\"PRAGMA table_info('{t}')\").fetchdf()[\"name\"].tolist()\n",
        "        columnas_union.update(cols)\n",
        "        columnas_por_tabla[t] = cols\n",
        "\n",
        "    columnas_union = sorted(list(columnas_union))\n",
        "    print(f\"üß± Total columnas √∫nicas detectadas: {len(columnas_union)}\\n\")\n",
        "\n",
        "    # --- Crear sentencias SELECT alineadas ---\n",
        "    selects = []\n",
        "    for t, cols in columnas_por_tabla.items():\n",
        "        # agregar NULL para columnas faltantes\n",
        "        select_cols = [f\"'{t}' AS origen\"]\n",
        "        for col in columnas_union:\n",
        "            if col in cols:\n",
        "                select_cols.append(f'\"{col}\"')\n",
        "            else:\n",
        "                select_cols.append(f\"NULL AS \\\"{col}\\\"\")\n",
        "        selects.append(f\"SELECT {', '.join(select_cols)} FROM '{t}'\")\n",
        "\n",
        "    union_query = \" UNION ALL \".join(selects)\n",
        "\n",
        "    # --- Crear tabla consolidada ---\n",
        "    con.execute(f\"\"\"\n",
        "        CREATE OR REPLACE TABLE consolidado_tic_4_1 AS {union_query}\n",
        "    \"\"\")\n",
        "    print(\"‚úÖ Tabla consolidada creada: consolidado_tic_4_1\")\n",
        "\n",
        "    # --- Mostrar resumen de consolidaci√≥n ---\n",
        "    resumen = con.execute(\"\"\"\n",
        "        SELECT origen, COUNT(*) AS registros\n",
        "        FROM consolidado_tic_4_1\n",
        "        GROUP BY origen\n",
        "        ORDER BY origen\n",
        "    \"\"\").fetchdf()\n",
        "\n",
        "    print(\"\\nüìä Registros por tabla origen:\")\n",
        "    display(resumen)\n",
        "\n",
        "    total = con.execute(\"SELECT COUNT(*) FROM consolidado_tic_4_1\").fetchone()[0]\n",
        "    print(f\"üßÆ Total de registros consolidados: {total:,}\")\n",
        "\n",
        "con.close()\n",
        "print(\"\\nüîö Conexi√≥n cerrada correctamente.\")\n"
      ],
      "metadata": {
        "id": "yYhtHCNIevlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque para explorar y visualizar la tabla consolidada"
      ],
      "metadata": {
        "id": "NemBxSt6fiFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C√≥digo optimizado para limpiar y renombrar consolidado_tic_4_1"
      ],
      "metadata": {
        "id": "_buD4dwYhcWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb, pandas as pd\n",
        "\n",
        "# --- Cerrar conexi√≥n previa si est√° abierta ---\n",
        "try:\n",
        "    con.close()\n",
        "    print(\"üîí Conexi√≥n anterior cerrada correctamente.\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è No hab√≠a conexi√≥n previa activa.\")\n",
        "\n",
        "# --- Reconexi√≥n a la base ---\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path)\n",
        "print(f\"‚úÖ Conectado a la base: {db_path}\")\n",
        "\n",
        "# --- Leer muestra para detectar encabezado correcto ---\n",
        "df_preview = con.execute(\"\"\"\n",
        "    SELECT * FROM consolidado_tic_4_1\n",
        "    LIMIT 10\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"\\nüëÄ Vista previa actual:\")\n",
        "display(df_preview)\n",
        "\n",
        "# --- Extraer nombres reales desde la fila 5 (√≠ndice 4) ---\n",
        "header_row = df_preview.iloc[4, 1:].tolist()\n",
        "header_row = [h.strip().upper() if isinstance(h, str) else f\"COLUMN_{i}\" for i, h in enumerate(header_row)]\n",
        "\n",
        "print(\"\\nüß≠ Encabezados detectados:\")\n",
        "print(header_row)\n",
        "\n",
        "# --- Cargar todo el dataset saltando las filas de metadatos (primeras 5 filas) ---\n",
        "df = con.execute(\"\"\"\n",
        "    SELECT * FROM consolidado_tic_4_1\n",
        "\"\"\").fetchdf().iloc[5:].copy()\n",
        "\n",
        "# --- Asignar nombres de columnas ---\n",
        "df.columns = [\"origen\"] + header_row\n",
        "\n",
        "# --- Limpiar filas vac√≠as o irrelevantes ---\n",
        "df = df.dropna(subset=[\"A√ëO\", \"DEPARTAMENTO\", \"MUNICIPIO\"], how=\"any\")\n",
        "\n",
        "# --- Convertir tipos num√©ricos ---\n",
        "cols_numericas = [\"A√ëO\", \"TRIMESTRE\", \"VELOCIDAD SUBIDA\", \"VELOCIDAD BAJADA\", \"No. ACCESOS FIJOS A INTERNET\"]\n",
        "for col in cols_numericas:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# --- Guardar tabla limpia ---\n",
        "con.register(\"df_clean\", df)\n",
        "con.execute(\"CREATE OR REPLACE TABLE consolidado_tic_4_1_limpia AS SELECT * FROM df_clean\")\n",
        "con.unregister(\"df_clean\")\n",
        "\n",
        "print(\"\\n‚úÖ Tabla 'consolidado_tic_4_1_limpia' creada correctamente.\")\n",
        "\n",
        "# --- Ver estructura y muestra ---\n",
        "estructura = con.execute(\"PRAGMA table_info('consolidado_tic_4_1_limpia')\").fetchdf()\n",
        "print(\"\\nüìã Nueva estructura:\")\n",
        "display(estructura)\n",
        "\n",
        "muestra = con.execute(\"SELECT * FROM consolidado_tic_4_1_limpia LIMIT 10\").fetchdf()\n",
        "print(\"\\nüëÄ Vista previa de datos limpios:\")\n",
        "display(muestra)\n",
        "\n",
        "# --- Cerrar conexi√≥n ---\n",
        "con.close()\n",
        "print(\"\\nüîö Conexi√≥n cerrada correctamente.\")\n"
      ],
      "metadata": {
        "id": "mMtR2-qMiGhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validaci√≥n de cobertura y consistencia"
      ],
      "metadata": {
        "id": "IGFfrLn2jCPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb, pandas as pd\n",
        "\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path, read_only=True)\n",
        "\n",
        "print(\"‚úÖ Conectado a la base para an√°lisis exploratorio.\\n\")\n",
        "\n",
        "# --- 1Ô∏è‚É£ Departamentos √∫nicos y conteo ---\n",
        "print(\"üìç Departamentos y n√∫mero de municipios reportados:\")\n",
        "deptos = con.execute(\"\"\"\n",
        "    SELECT DEPARTAMENTO, COUNT(DISTINCT MUNICIPIO) AS municipios_unicos, COUNT(*) AS total_registros\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "    GROUP BY DEPARTAMENTO\n",
        "    ORDER BY DEPARTAMENTO\n",
        "\"\"\").fetchdf()\n",
        "display(deptos)\n",
        "\n",
        "# --- 2Ô∏è‚É£ Cobertura temporal ---\n",
        "print(\"\\nüóìÔ∏è A√±os y trimestres disponibles:\")\n",
        "periodos = con.execute(\"\"\"\n",
        "    SELECT A√ëO, TRIMESTRE, COUNT(*) AS registros\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "    GROUP BY A√ëO, TRIMESTRE\n",
        "    ORDER BY A√ëO, TRIMESTRE\n",
        "\"\"\").fetchdf()\n",
        "display(periodos)\n",
        "\n",
        "# --- 3Ô∏è‚É£ Validaci√≥n de velocidades ---\n",
        "print(\"\\nüìà Rango de velocidades detectadas:\")\n",
        "velocidades = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        MIN(\"VELOCIDAD SUBIDA\") AS min_subida,\n",
        "        MAX(\"VELOCIDAD SUBIDA\") AS max_subida,\n",
        "        MIN(\"VELOCIDAD BAJADA\") AS min_bajada,\n",
        "        MAX(\"VELOCIDAD BAJADA\") AS max_bajada,\n",
        "        ROUND(AVG(\"VELOCIDAD SUBIDA\"),2) AS promedio_subida,\n",
        "        ROUND(AVG(\"VELOCIDAD BAJADA\"),2) AS promedio_bajada\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "\"\"\").fetchdf()\n",
        "display(velocidades)\n",
        "\n",
        "con.close()\n",
        "print(\"\\nüîö Conexi√≥n cerrada correctamente.\")\n"
      ],
      "metadata": {
        "id": "JY4OY2jijDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "con = duckdb.connect(\"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\")\n",
        "con.execute(\"SHOW TABLES\").fetchdf()\n"
      ],
      "metadata": {
        "id": "uYynhkWrkn3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificaci√≥n de unicidad"
      ],
      "metadata": {
        "id": "FwxbfdsulB3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "con = duckdb.connect(\"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\")\n",
        "\n",
        "duplicados = con.execute(\"\"\"\n",
        "    SELECT COUNT(*) AS total_registros,\n",
        "           COUNT(DISTINCT (A√ëO, TRIMESTRE, DEPARTAMENTO, MUNICIPIO, PROVEEDOR, \"TECNOLOG√çA\")) AS registros_unicos\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(duplicados)\n",
        "con.close()\n"
      ],
      "metadata": {
        "id": "CoaTrGxelA98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consulta para verificar dimensiones y muestra de datos"
      ],
      "metadata": {
        "id": "sKZzwFMqls7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "# --- Conexi√≥n a la base ---\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path)\n",
        "\n",
        "# --- 1Ô∏è‚É£ Dimensiones de la tabla ---\n",
        "dim = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) AS total_registros,\n",
        "        COUNT(DISTINCT DEPARTAMENTO) AS departamentos,\n",
        "        COUNT(DISTINCT MUNICIPIO) AS municipios,\n",
        "        COUNT(DISTINCT A√ëO) AS anios,\n",
        "        COUNT(DISTINCT TRIMESTRE) AS trimestres,\n",
        "        COUNT(DISTINCT PROVEEDOR) AS proveedores,\n",
        "        COUNT(DISTINCT \"TECNOLOG√çA\") AS tecnologias\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"üìè Dimensiones de la tabla `consolidado_tic_4_1_limpia`:\")\n",
        "display(dim)\n",
        "\n",
        "# --- 2Ô∏è‚É£ Vista previa ordenada por a√±o y trimestre ---\n",
        "muestra = con.execute(\"\"\"\n",
        "    SELECT A√ëO, TRIMESTRE, DEPARTAMENTO, MUNICIPIO, PROVEEDOR,\n",
        "           \"SEGMENTO\", \"TECNOLOG√çA\", \"VELOCIDAD SUBIDA\", \"VELOCIDAD BAJADA\",\n",
        "           \"NO. ACCESOS FIJOS A INTERNET\"\n",
        "    FROM consolidado_tic_4_1_limpia\n",
        "    WHERE A√ëO IS NOT NULL\n",
        "    ORDER BY A√ëO DESC, TRIMESTRE DESC\n",
        "    LIMIT 20\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"\\nüëÄ Vista previa de registros representativos:\")\n",
        "display(muestra)\n",
        "\n",
        "con.close()\n",
        "print(\"\\nüîö Conexi√≥n cerrada correctamente.\")\n"
      ],
      "metadata": {
        "id": "TfMF_KWEltci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de detecci√≥n y limpieza de outliers"
      ],
      "metadata": {
        "id": "EuOkHdNrowNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "db_path = \"/content/gdrive/MyDrive/trabajoGrado/colombiatic_datos/colombiatic.duckdb\"\n",
        "con = duckdb.connect(db_path)\n",
        "\n",
        "# --- Cargar los datos a memoria para depuraci√≥n ---\n",
        "df = con.execute(\"SELECT * FROM consolidado_tic_4_1_limpia\").fetchdf()\n",
        "\n",
        "print(f\"üìä Registros iniciales: {len(df):,}\")\n",
        "\n",
        "# --- Convertir a num√©rico por seguridad ---\n",
        "for col in [\"VELOCIDAD SUBIDA\", \"VELOCIDAD BAJADA\"]:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# --- Calcular IQR para cada variable ---\n",
        "def limpiar_outliers_iqr(serie):\n",
        "    q1 = serie.quantile(0.25)\n",
        "    q3 = serie.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    limite_inferior = q1 - 1.5 * iqr\n",
        "    limite_superior = q3 + 1.5 * iqr\n",
        "    return serie.clip(lower=limite_inferior, upper=limite_superior)\n",
        "\n",
        "df[\"VELOCIDAD SUBIDA_LIMPIA\"] = limpiar_outliers_iqr(df[\"VELOCIDAD SUBIDA\"])\n",
        "df[\"VELOCIDAD BAJADA_LIMPIA\"] = limpiar_outliers_iqr(df[\"VELOCIDAD BAJADA\"])\n",
        "\n",
        "# --- Conteo de valores corregidos ---\n",
        "cambios_subida = (df[\"VELOCIDAD SUBIDA\"] != df[\"VELOCIDAD SUBIDA_LIMPIA\"]).sum()\n",
        "cambios_bajada = (df[\"VELOCIDAD BAJADA\"] != df[\"VELOCIDAD BAJADA_LIMPIA\"]).sum()\n",
        "\n",
        "print(f\"‚öôÔ∏è Registros ajustados (subida): {cambios_subida:,}\")\n",
        "print(f\"‚öôÔ∏è Registros ajustados (bajada): {cambios_bajada:,}\")\n",
        "\n",
        "# --- Guardar tabla limpia ---\n",
        "con.execute(\"DROP TABLE IF EXISTS consolidado_tic_4_1_filtrado\")\n",
        "con.register(\"df_temp\", df)\n",
        "con.execute(\"\"\"\n",
        "    CREATE TABLE consolidado_tic_4_1_filtrado AS\n",
        "    SELECT\n",
        "        origen, A√ëO, TRIMESTRE, DEPARTAMENTO, MUNICIPIO, PROVEEDOR,\n",
        "        SEGMENTO, TECNOLOG√çA,\n",
        "        \"NO. ACCESOS FIJOS A INTERNET\",\n",
        "        \"C√ìDIGO DANE\", \"C√ìDIGO DANE_1\",\n",
        "        \"VELOCIDAD SUBIDA_LIMPIA\" AS VELOCIDAD_SUBIDA,\n",
        "        \"VELOCIDAD BAJADA_LIMPIA\" AS VELOCIDAD_BAJADA\n",
        "    FROM df_temp\n",
        "\"\"\")\n",
        "con.unregister(\"df_temp\")\n",
        "\n",
        "# --- Verificar dimensiones ---\n",
        "dim = con.execute(\"SELECT COUNT(*) AS total, COUNT(DISTINCT MUNICIPIO) AS municipios FROM consolidado_tic_4_1_filtrado\").fetchdf()\n",
        "print(\"\\n‚úÖ Tabla `consolidado_tic_4_1_filtrado` creada correctamente:\")\n",
        "display(dim)\n",
        "\n",
        "con.close()\n",
        "print(\"\\nüîö Conexi√≥n cerrada correctamente.\")\n"
      ],
      "metadata": {
        "id": "bGO8JFs7o5dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
